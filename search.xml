<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[Kubernetes 搭建ingress]]></title>
      <url>%2F2017%2F04%2F24%2FKubernetes-%E6%90%AD%E5%BB%BAingress%2F</url>
      <content type="text"><![CDATA[优秀的人，不是不合群，而是他们合群的人里面没有你]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Kubernetes 配置HPA，实现动态扩容]]></title>
      <url>%2F2017%2F04%2F24%2FKubernetes-%E9%85%8D%E7%BD%AEHPA%EF%BC%8C%E5%AE%9E%E7%8E%B0%E5%8A%A8%E6%80%81%E6%89%A9%E5%AE%B9%2F</url>
      <content type="text"><![CDATA[优秀的人，不是不合群，而是他们合群的人里面没有你 测试方案一(CPU限制)下载必要的yaml文件https://github.com/kubernetes/kubernetes/tree/8caeec429ee1d2a9df7b7a41b21c626346b456fb/docs/user-guide/horizontal-pod-autoscaling 修改官方的规则官方的方案是基于RC的，现在我们更换成Deployment的方式。 cat php-apache-Deployment.yaml cat php-apache-Service.yaml cat php-apache-Hpa.yaml 创建Podkubectl create -f /home/kube-yaml/pod-autoscaler/ 查看集群情况 注：k8s是基于周期性采样进行处理扩容和收缩的。所以，并不是根据cup或者memory的变化，立马做出扩容的处理(需要几分钟的时间)。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Kubernetes 搭建grafana+heapster+influxdb监控服务]]></title>
      <url>%2F2017%2F04%2F24%2FKubernetes-%E6%90%AD%E5%BB%BAgrafana-heapster-influxdb%E7%9B%91%E6%8E%A7%E6%9C%8D%E5%8A%A1%2F</url>
      <content type="text"><![CDATA[优秀的人，不是不合群，而是他们合群的人里面没有你 下载必要的yaml文件到github上面进行下载deploy/kube-config/influxdb 修改Yaml文件信息替换镜像来源12345678gcr.io/google_containers/heapster-grafana:v4.0.2registry.cn-hangzhou.aliyuncs.com/kube_containers/heapster_grafanagcr.io/google_containers/heapster-influxdb:v1.1.1registry.cn-hangzhou.aliyuncs.com/kube_containers/heapster_influxdbgcr.io/google_containers/heapster:v1.3.0-beta.0registry.cn-hangzhou.aliyuncs.com/wayne/heapster:v1.1.0 替换内容部分cat grafana-deployment.yaml cat grafana-service.yaml cat heapster-deployment.yaml cat /etc/kubernetes/test-kubeconfig.yaml cat heapster-service.yaml cat influxdb-deployment.yaml cat influxdb-service.yaml 创建podkubectl create -f /home/kube-yaml/grafana-heapster-influxdb 查看镜像运行情况 查看界面]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Kubernetes 搭建jenkins]]></title>
      <url>%2F2017%2F04%2F24%2FKubernetes-%E6%90%AD%E5%BB%BAjenkins%2F</url>
      <content type="text"><![CDATA[优秀的人，不是不合群，而是他们合群的人里面没有你 创建项目 配置信息 配置编译和生成镜像文件 启动界面 运行结果]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Kubernetes 搭建kube-dns]]></title>
      <url>%2F2017%2F04%2F24%2FKubernetes-%E6%90%AD%E5%BB%BAkube-dns%2F</url>
      <content type="text"><![CDATA[优秀的人，不是不合群，而是他们合群的人里面没有你 第一种方案(容器部署方案)拉取官方kube-dns yaml文件wget https://rawgit.com/kubernetes/kubernetes/release-1.5/cluster/addons/dns/skydns-rc.yaml.sed -O skydns-rc.yaml wget https://rawgit.com/kubernetes/kubernetes/release-1.5/cluster/addons/dns/skydns-svc.yaml.sed -O skydns-svc.yaml 编辑yaml文件编辑 skydns-rc.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197# Copyright 2016 The Kubernetes Authors.## Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# TODO - At some point, we need to rename all skydns-*.yaml.* files to kubedns-*.yaml.*# Should keep target in cluster/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml# in sync with this file.# Warning: This is a file generated from the base underscore template file: skydns-rc.yaml.baseapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: kube-dns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: "true"spec: #指定副本数 replicas: 1 # replicas: not specified here: # 1. In order to make Addon Manager do not reconcile this replicas parameter. # 2. Default is 1. # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on. strategy: rollingUpdate: maxSurge: 10% maxUnavailable: 0 selector: matchLabels: k8s-app: kube-dns template: metadata: labels: k8s-app: kube-dns annotations: scheduler.alpha.kubernetes.io/critical-pod: '' scheduler.alpha.kubernetes.io/tolerations: '[&#123;"key":"CriticalAddonsOnly", "operator":"Exists"&#125;]' spec: containers: - name: kubedns image: 192.168.204.66/google_containers/kubedns-amd64:1.9 resources: # TODO: Set memory limits when we've profiled the container for large # clusters, then set request = limit to keep this container in # guaranteed class. Currently, this container falls into the # "burstable" category so the kubelet doesn't backoff from restarting it. limits: memory: 170Mi requests: cpu: 100m memory: 70Mi livenessProbe: httpGet: path: /healthz-kubedns port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 readinessProbe: httpGet: path: /readiness port: 8081 scheme: HTTP # we poll on pod startup for the Kubernetes master service and # only setup the /readiness HTTP server once that's available. initialDelaySeconds: 3 timeoutSeconds: 5 args: # --domain指定一级域名，可自定义 # - --domain=$DNS_DOMAIN. - --domain=cluster.local #设置k8s集群中Service所属的域名 - --dns-port=10053 - --config-map=kube-dns # 增加--kube-master-url，指向kube master的地址 - --kube-master-url=https://10.200.102.93:6443 #k8s中master的ip地址和apiserver中配置的端口号 - --kubecfg_file=/etc/kubernetes/worker-kubeconfig.yaml # This should be set to v=2 only after the new image (cut from 1.5) has # been released, otherwise we will flood the logs. - --v=0 volumeMounts: - mountPath: /etc/kubernetes/ssl name: ssl-certs-kubernetes - mountPath: /etc/ssl/certs name: ssl-certs-host - mountPath: /etc/kubernetes/worker-kubeconfig.yaml name: config env: - name: PROMETHEUS_PORT value: "10055" ports: - containerPort: 10053 name: dns-local protocol: UDP - containerPort: 10053 name: dns-tcp-local protocol: TCP - containerPort: 10055 name: metrics protocol: TCP - name: dnsmasq image: 192.168.204.66/google_containers/kube-dnsmasq-amd64:1.4 livenessProbe: httpGet: path: /healthz-dnsmasq port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 args: - --cache-size=1000 - --no-resolv - --server=127.0.0.1#10053 # - --log-facility=- ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP # see: https://github.com/kubernetes/kubernetes/issues/29055 for details resources: requests: cpu: 150m memory: 10Mi - name: dnsmasq-metrics image: 192.168.204.66/google_containers/dnsmasq-metrics-amd64:1.0 livenessProbe: httpGet: path: /metrics port: 10054 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 args: - --v=2 - --logtostderr ports: - containerPort: 10054 name: metrics protocol: TCP resources: requests: memory: 10Mi - name: healthz image: 192.168.204.66/google_containers/exechealthz-amd64:1.2 resources: limits: memory: 50Mi requests: cpu: 10m # Note that this container shouldn't really need 50Mi of memory. The # limits are set higher than expected pending investigation on #29688. # The extra memory was stolen from the kubedns container to keep the # net memory requested by the pod constant. memory: 50Mi args: - --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1 &gt;/dev/null - --url=/healthz-dnsmasq - --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1:10053 &gt;/dev/null - --url=/healthz-kubedns - --port=8080 - --quiet ports: - containerPort: 8080 protocol: TCP dnsPolicy: Default # Don't use cluster DNS. volumes: - name: etcd-storage emptyDir: &#123;&#125; - hostPath: path: /etc/kubernetes/ssl name: ssl-certs-kubernetes - hostPath: path: /etc/pki/tls/certs name: ssl-certs-host - hostPath: path: /etc/kubernetes/worker-kubeconfig.yaml name: config 编辑skydns-svc.yaml文件1234567891011121314151617181920212223242526272829303132333435363738[root@k8s-master kube-yaml]# cat sky-dns/skydns-svc.yaml # Copyright 2016 The Kubernetes Authors.## Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# TODO - At some point, we need to rename all skydns-*.yaml.* files to kubedns-*.yaml.*# Warning: This is a file generated from the base underscore template file: skydns-svc.yaml.baseapiVersion: v1kind: Servicemetadata: name: kube-dns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: "true" kubernetes.io/name: "KubeDNS"spec: selector: k8s-app: kube-dns clusterIP: 172.30.0.3 #/etc/kubernetes/kubelet中已经设定好clusterIP ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 运行yaml文件123[root@seanzhau.com ~]# kubectl create -f skydns-rc.yaml[root@seanzhau.com ~]# kubectl create -f skydns-svc.yaml[root@seanzhau.com ~]# kubectl create -f busybox.yaml 运行结果 第二种方案(插件/命令部署方案)该方案目前尚未寻找到支持TLS的方案实现，但木有已经实现了无数字证书验证的方案。 下载kube-dns命令123# wget https://dl.k8s.io/v1.5.2/kubernetes-server-linux-amd64.tar.gz# tar -xf kubernetes-server-linux-amd64.tar.gz# mv /opt/docker/src/kubernetes/server/bin/kube-dns /usr/bin/ 新建kube-dns配置文件12345# vi /etc/kubernetes/kube-dnsKUBE_DNS_PORT="--dns-port=53"KUBE_DNS_DOMAIN="--domain=cluster.local"KUBE_DNS_MASTER=--kube-master-url="http://192.168.40.50:8080"KUBE_DNS_ARGS="" 新建kube-dns.service配置文件123456789101112131415161718[root@k8s-master calico]# cat /usr/lib/systemd/system/kube-dns.service [Unit]Description=Kubernetes Kube-dns ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=kube-apiserver.serviceRequires=kube-apiserver.service [Service]WorkingDirectory=/var/lib/kube-dnsEnvironmentFile=-/etc/kubernetes/kube-dnsExecStart=/usr/bin/kube-dns \ $KUBE_DNS_PORT \ $KUBE_DNS_DOMAIN \ $KUBE_DNS_MASTER \ $KUBE_DNS_ARGSRestart=on-failure[Install]WantedBy=multi-user.target master启动123# mkdir -p /var/lib/kube-dns# systemctl enable kube-dns# systemctl restart kube-dns master修改/etc/resolv.conf文件123456[root@k8s-master calico]# cat /etc/resolv.conf# Generated by NetworkManagersearch default.svc.cluster.local svc.cluster.local cluster.localnameserver 10.200.102.95nameserver 223.5.5.5nameserver 202.96.128.86 node结点修改kubelet文件1234567891011121314151617181920[root@k8s-node-1 ~]# cat /etc/kubernetes/kubelet #### kubernetes kubelet (minion) config# The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)#KUBELET_ADDRESS="--address=127.0.0.1"KUBELET_ADDRESS="--address=0.0.0.0"# The port for the info server to serve on# KUBELET_PORT="--port=10250"# You may leave this blank to use the actual hostname#KUBELET_HOSTNAME="--hostname-override=127.0.0.1"KUBELET_HOSTNAME="--hostname-override=k8s-node-1"# location of the api-server#KUBELET_API_SERVER="--api-servers=http://127.0.0.1:8080"KUBELET_API_SERVER="--api-servers=http://10.200.102.95:8080"# pod infrastructure container#KUBELET_POD_INFRA_CONTAINER="--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest"KUBELET_POD_INFRA_CONTAINER="--pod-infra-container-image=192.168.204.66/google_containers/pause:latest"# Add your own!KUBELET_ARGS="--cluster-dns=10.200.102.95 --cluster-domain=cluster.local" 检测123456[root@k8s-master kube]# kubectl exec -ti busybox --namespace=kube-system -- nslookup kubernetes.defaultServer: 10.200.102.95Address 1: 10.200.102.95Name: kubernetes.defaultAddress 1: 10.254.0.1 kubernetes.default.svc.cluster.local]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Kubernetes 搭建dashboard]]></title>
      <url>%2F2017%2F04%2F21%2FKubernetes-%E6%90%AD%E5%BB%BAdashboard%2F</url>
      <content type="text"><![CDATA[优秀的人，不是不合群，而是他们合群的人里面没有你 拉去官方的rc文件(Deployment)相关的拉取路径：kubectl create -f https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253kind: DeploymentapiVersion: extensions/v1beta1metadata: labels: app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: replicas: 1 selector: matchLabels: app: kubernetes-dashboard template: metadata: labels: app: kubernetes-dashboard spec: containers: - name: kubernetes-dashboard image: index.tenxcloud.com/google_containers/kubernetes-dashboard-amd64:v1.4.1 imagePullPolicy: IfNotPresent ports: - containerPort: 9090 protocol: TCP args: # Uncomment the following line to manually specify Kubernetes API server Host # If not specified, Dashboard will attempt to auto discover the API server and connect # to it. Uncomment only if the default does not work. - --apiserver-host=https://10.200.102.93:6443 - --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml livenessProbe: httpGet: path: / port: 9090 initialDelaySeconds: 30 timeoutSeconds: 30 volumeMounts: - mountPath: /etc/kubernetes/ssl name: ssl-certs-kubernetes - mountPath: /etc/ssl/certs name: ssl-certs-host - mountPath: /etc/kubernetes/worker-kubeconfig.yaml name: config volumes: - hostPath: path: /etc/kubernetes/ssl name: ssl-certs-kubernetes - hostPath: path: /etc/pki/tls/certs name: ssl-certs-host - hostPath: path: /etc/kubernetes/worker-kubeconfig.yaml name: config 对应的Service文件123456789101112131415kind: ServiceapiVersion: v1metadata: labels: app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: type: NodePort ports: - port: 80 nodePort: 30010 targetPort: 9090 selector: app: kubernetes-dashboard 执行rc文件1kubectl create -f kubernetes-dashboard.yaml 查看pod状态 访问地址：nodeIP：NodePort 界面]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Kubernetes 双向证书TLS配置]]></title>
      <url>%2F2017%2F04%2F21%2FKubernetes-%E5%8F%8C%E5%90%91%E8%AF%81%E4%B9%A6TLS%E9%85%8D%E7%BD%AE%2F</url>
      <content type="text"><![CDATA[优秀的人，不是不合群，而是他们合群的人里面没有你 签发证书TLS双向认知需要预先自建CA签发证书，权威CA机构的证书应该不可用，因为大部分k8s都是在内网中部署，而内网应该都会采用私有IP地址通讯，权威CA好像只能签署域名证书，对签署到IP可能无法实现. 自签CA对于私有证书签发首先要自签署一个CA根证书创建证书存放的目录,创建CA私钥,自签CA12345[root@master ~]# mkdir /etc/kubernetes/ssl &amp;&amp; cd /etc/kubernetes/sslopenssl genrsa -out ca-key.pem 2048[root@master ssl]# openssl req -x509 -new -nodes -key ca-key.pem -days 10000 -out ca.pem -subj "/CN=kube-ca"[root@master ssl]# lsca-key.pem ca.pem 签署apiserver 证书自签 CA 后就需要使用这个根 CA 签署 apiserver 相关的证书了，首先先修改 openssl 的配置。12345678910111213141516# vim openssl.cnf[req]req_extensions = v3_reqdistinguished_name = req_distinguished_name[req_distinguished_name][ v3_req ]basicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEnciphermentsubjectAltName = @alt_names[alt_names]DNS.1 = kubernetesDNS.2 = kubernetes.defaultDNS.3 = kubernetes.default.svcDNS.4 = kubernetes.default.svc.cluster.localIP.1 = 10.254.0.1 #k8s 集群service ip(dns),关键地方IP.2 = 10.200.102.93 #k8s master ip 然后开始签署apiserver相关的证书123456# 生成 apiserver 私钥openssl genrsa -out apiserver-key.pem 2048# 生成签署请求openssl req -new -key apiserver-key.pem -out apiserver.csr -subj "/CN=kube-apiserver" -config openssl.cnf# 使用自建 CA 签署openssl x509 -req -in apiserver.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out apiserver.pem -days 365 -extensions v3_req -extfile openssl.cnf 生成集群管理证书123openssl genrsa -out admin-key.pem 2048openssl req -new -key admin-key.pem -out admin.csr -subj "/CN=kube-admin"openssl x509 -req -in admin.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out admin.pem -days 365 签署node证书1234567891011121314[root@master ssl]# cp openssl.cnf worker-openssl.cnf[root@master ssl]# cat worker-openssl.cnf [req]req_extensions = v3_reqdistinguished_name = req_distinguished_name[req_distinguished_name][ v3_req ]basicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEnciphermentsubjectAltName = @alt_names[alt_names]IP.1 = 10.200.102.92IP.2 = 10.200.102.81IP.3 = 10.200.102.82 生成各个结点的证书并且拷贝到每个节点的目录下12345678[root@master ssl]# for i in &#123;k8s-master,k8s-node-1,k8s-node-2,k8s-node-3&#125;&gt; do&gt; openssl genrsa -out $i-worker-key.pem 2048&gt; openssl req -new -key $i-worker-key.pem -out $i-worker.csr -subj "/CN=$i" -config worker-openssl.cnf&gt; openssl x509 -req -in $i-worker.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out $i-worker.pem -days 365 -extensions v3_req -extfile worker-openssl.cnf&gt; ssh root@$i "mkdir /etc/kubernetes/ssl;chown kube:kube -R /etc/kubernetes/ssl"&gt; scp /etc/kubernetes/ssl/ca.pem /etc/kubernetes/ssl/$i* root@$i:/etc/kubernetes/ssl&gt; done 配置k8s配置masterapiserver文件123456789101112131415161718192021KUBE_API_ADDRESS="--bind-address=10.200.102.93 --insecure-bind-address=127.0.0.1 "# The port on the local server to listen on.KUBE_API_PORT=="--secure-port=6443 --insecure-port=8080"# Port minions listen on# KUBELET_PORT="--kubelet-port=10250"# Comma separated list of nodes in the etcd clusterKUBE_ETCD_SERVERS="--etcd-servers=http://10.200.102.93:2379"# Address range to use for servicesKUBE_SERVICE_ADDRESSES="--service-cluster-ip-range=10.254.0.0/16"# default admission control policiesKUBE_ADMISSION_CONTROL="--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota"# Add your own!KUBE_API_ARGS="--tls-cert-file=/etc/kubernetes/ssl/apiserver.pem \ --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem \ --client-ca-file=/etc/kubernetes/ssl/ca.pem \ --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem" config文件1234567891011121314151617181920212223#### kubernetes system config## The following values are used to configure various aspects of all# kubernetes services, including## kube-apiserver.service# kube-controller-manager.service# kube-scheduler.service# kubelet.service# kube-proxy.service# logging to stderr means we get it in the systemd journalKUBE_LOGTOSTDERR="--logtostderr=true"# journal message level, 0 is debugKUBE_LOG_LEVEL="--v=0"# Should this cluster be allowed to run privileged docker containersKUBE_ALLOW_PRIV="--allow-privileged=false"# How the controller-manager, scheduler, and proxy find the apiserver#KUBE_MASTER="--master=http://k8s-master:8080"KUBE_MASTER="--master=https://k8s-master:6443" scheduler文件12345678[root@k8s-master kube-yaml]# cat /etc/kubernetes/scheduler #### kubernetes scheduler config# default config should be adequate# Add your own!KUBE_SCHEDULER_ARGS="--kubeconfig=/etc/kubernetes/cm-kubeconfig.yaml --master=http://127.0.0.1:8080" controller-manager文件123456789[root@k8s-master kube-yaml]# cat /etc/kubernetes/controller-manager #### The following values are used to configure the kubernetes controller-manager# defaults from config and apiserver should be adequate# Add your own!KUBE_CONTROLLER_MANAGER_ARGS="--service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem \ --root-ca-file=/etc/kubernetes/ssl/ca.pem \ --master=http://127.0.0.1:8080 \ --kubeconfig=/etc/kubernetes/cm-kubeconfig.yaml" 创建一个/etc/kubernetes/cm-kubeconfig.yaml 文件1234567891011121314151617apiVersion: v1kind: Configclusters:- name: local cluster: certificate-authority: /etc/kubernetes/ssl/ca.pemusers:- name: controllermanager user: client-certificate: /etc/kubernetes/ssl/apiserver.pem client-key: /etc/kubernetes/ssl/apiserver-key.pemcontexts:- context: cluster: local user: controllermanager name: kubelet-contextcurrent-context: kubelet-context 重启服务1systemctl restart etcd kube-apiserver.service kube-controller-manager.service kube-scheduler.service 配置node结点（以node1为例子）config文件123456789101112131415161718192021222324[root@k8s-node-1 ~]# cat /etc/kubernetes/config #### kubernetes system config## The following values are used to configure various aspects of all# kubernetes services, including## kube-apiserver.service# kube-controller-manager.service# kube-scheduler.service# kubelet.service# kube-proxy.service# logging to stderr means we get it in the systemd journalKUBE_LOGTOSTDERR="--logtostderr=true"# journal message level, 0 is debugKUBE_LOG_LEVEL="--v=0"# Should this cluster be allowed to run privileged docker containersKUBE_ALLOW_PRIV="--allow-privileged=false"# How the controller-manager, scheduler, and proxy find the apiserver#KUBE_MASTER="--master=http://127.0.0.1:8080"KUBE_MASTER="--master=https://10.200.102.93:6443" kubelet文件12345678910111213141516171819202122232425262728[root@k8s-node-1 ~]# cat /etc/kubernetes/kubelet#### kubernetes kubelet (minion) config# The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)#KUBELET_ADDRESS="--address=0.0.0.0"KUBELET_ADDRESS="--address=10.200.102.92"# The port for the info server to serve onKUBELET_PORT="--port=10250"# You may leave this blank to use the actual hostnameKUBELET_HOSTNAME="--hostname-override=k8s-node-1"# location of the api-server#KUBELET_API_SERVER="--api-servers=http://k8s-master:8080"KUBELET_API_SERVER="--api-servers=https://10.200.102.93:6443"# pod infrastructure container#KUBELET_POD_INFRA_CONTAINER="--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest"KUBELET_POD_INFRA_CONTAINER="--pod-infra-container-image=192.168.204.66/google_containers/pause:latest"# Add your own!KUBELET_ARGS="--cluster_dns=172.30.0.3 \ --cluster_domain=cluster.local \ --tls-cert-file=/etc/kubernetes/ssl/k8s-node-1-worker.pem \ --tls-private-key-file=/etc/kubernetes/ssl/k8s-node-1-worker-key.pem \ --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml \ --allow-privileged=true " proxy文件12345678[root@k8s-node-1 ~]# cat /etc/kubernetes/proxy #### kubernetes proxy config# default config should be adequate# Add your own!KUBE_PROXY_ARGS="--proxy-mode=iptables \ --master=https://10.200.102.93:6443 \ --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml" 创建一个文件worker-kubeconfig.yaml123456789101112131415161718apiVersion: v1kind: Configclusters:- name: local cluster: server: https://10.200.102.93:6443 certificate-authority: /etc/kubernetes/ssl/ca.pemusers:- name: kubelet user: client-certificate: /etc/kubernetes/ssl/node1-worker.pem client-key: /etc/kubernetes/ssl/node1-worker-key.pemcontexts:- context: cluster: local user: kubelet name: kubelet-contextcurrent-context: kubelet-context 123456node1#重启服务systemctl restart kubelet kube-proxy#查看 状态systemctl status kubelet kube-proxy -l 验证TLS12#验证证书curl https://10.200.102.93:6443/api/v1/nodes --cert /etc/kubernetes/ssl/k8s-node-1-worker.pem --key /etc/kubernetes/ssl/k8s-node-1-worker-key.pem --cacert /etc/kubernetes/ssl/ca.pem]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Kubernetes环境搭建]]></title>
      <url>%2F2017%2F04%2F20%2FKubernetes%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
      <content type="text"><![CDATA[优秀的人，不是不合群，而是他们合群的人里面没有你 第一种安装方案(官网)基于官方的安装方式(安装包并非是最新版本的) 准备CentOS 7.x环境查看内核版本123[root@k8s-master kube-yaml]# uname -r3.10.0-514.6.1.el7.x86_64[root@k8s-master kube-yaml]# 最好是选择3.10版本以上的内核，进行安装。本次安装，选择了4台服务器进行集群安装。 123456[root@k8s-master kube-yaml]# cat /etc/hosts10.200.102.93 k8s-master10.200.102.92 k8s-node-110.200.102.81 k8s-node-210.200.102.82 k8s-node-3[root@k8s-master kube-yaml]# 配置官方k8s yum源:1234[virt7-docker-common-release]name=virt7-docker-common-releasebaseurl=http://cbs.centos.org/repos/virt7-docker-common-release/x86_64/os/gpgcheck=0 配置阿里云yum源:123456789101112131415161718192021222324252627282930313233343536373839404142434445[base]name=CentOS-$releasever - Base - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos/$releasever/os/$basearch/ http://mirrors.aliyuncs.com/centos/$releasever/os/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=osgpgcheck=1gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7#released updates[updates]name=CentOS-$releasever - Updates - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos/$releasever/updates/$basearch/ http://mirrors.aliyuncs.com/centos/$releasever/updates/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=updatesgpgcheck=1gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7#additional packages that may be useful[extras]name=CentOS-$releasever - Extras - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos/$releasever/extras/$basearch/http://mirrors.aliyuncs.com/centos/$releasever/extras/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=extrasgpgcheck=1gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7#additional packages that extend functionality of existing packages[centosplus]name=CentOS-$releasever - Plus - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos/$releasever/centosplus/$basearch/ http://mirrors.aliyuncs.com/centos/$releasever/centosplus/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=centosplusgpgcheck=1enabled=0gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7#contrib - packages by Centos Users[contrib]name=CentOS-$releasever - Contrib - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos/$releasever/contrib/$basearch/http://mirrors.aliyuncs.com/centos/$releasever/contrib/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=contribgpgcheck=1enabled=0gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7 更新本地镜像源12yum clean allyum makecache 安装Kubernetes环境(Master)1yum -y install --enablerepo=virt7-docker-common-release kubernetes etcd flannel安装的过程有点久，因为需要下载和安装。期间如果出现什么下载失败，更新包更新失败。基本上都是因为yum的问题，换个国内大企业的镜像yum就好了。至此，整个下载和安装的过程就算成功了。编辑本地host文件，做好访问映射：vim /etc/hosts1234567[root@k8s-master kube-yaml]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain610.200.102.93 k8s-master10.200.102.92 k8s-node-110.200.102.81 k8s-node-210.200.102.82 k8s-node-3编辑k8s的配置文件信息：vi /etc/kubernetes/config由于CentOS 7.x默认是开启防火墙的，需要进行防火墙的设置操作：关闭SELinux：setenforce 01、临时关闭(不用重启机器):setenforce 0 #设置SELinux 成为permissive模式setenforce 1 #设置SELinux 成为enforcing模式2、关闭防火墙:12systemctl stop firewalld.servicesystemctl disable firewalld.service 编辑etcd的配置文件信息：vi /etc/etcd/etcd.conf 编辑k8s的配置信息：vi /etc/kubernetes/apiserver 启动etcd服务：systemctl start etcd 创建网络，并且设置网络配置信息：123etcdctl mkdir /kube-centos/networketcdctl mk /kube-centos/network/config "&#123;\"Network\":\"172.30.0.0/16\",\"SubnetLen\":24,\"Backend\":&#123;\"Type\":\"vxlan\"&#125;&#125;" 配置flanneld信息：vi /etc/sysconfig/flanneld 运行环境：for SERVICES in etcd kube-apiserver kube-controller-manager kube-scheduler flanneld; do systemctl restart $SERVICES; systemctl enable $SERVICES; systemctl status $SERVICES; done 至此k8s-master的安装和启动到此完成。 安装Kubernetes环境(Minion/Node):内核版本和yum源配置，请参考上面部分进行配置就可以了。1yum -y install --enablerepo=virt7-docker-common-release kubernetes flannel 安装的过程有点久，因为需要下载和安装。期间如果出现什么下载失败，更新包更新失败。基本上都是因为yum的问题，换个国内大企业的镜像yum就好了。至此，整个下载和安装的过程就算成功了。 编辑本地host文件，做好访问映射：vim /etc/hosts1234567[root@k8s-master kube-yaml]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain610.200.102.93 k8s-master10.200.102.92 k8s-node-110.200.102.81 k8s-node-210.200.102.82 k8s-node-3 由于CentOS 7.x默认是开启防火墙的，需要进行防火墙的设置操作：关闭SELinux：setenforce 01、临时关闭(不用重启机器): setenforce 0 #设置SELinux 成为permissive模式setenforce 1 #设置SELinux 成为enforcing模式2、关闭防火墙:12systemctl stop firewalld.servicesystemctl disable firewalld.service 编辑k8s的配置文件信息：vi /etc/kubernetes/config 配置kubernetes信息：vi /etc/kubernetes/kubelet 配置flanneld信息：vi /etc/sysconfig/flanneld 运行环境：for SERVICES in kube-proxy kubelet flanneld docker; do systemctl restart $SERVICES; systemctl enable $SERVICES; systemctl status $SERVICES; done 配置参数：123kubectl config set-cluster default-cluster --server=http://k8s-master:8080kubectl config set-context default-context --cluster=default-cluster --user=default-adminkubectl config use-context default-context 第二种安装方案(tar安装)服务器环境：123456710.15.206.120 vip 10.15.206.105 master 10.15.206.106 node 10.15.206.107 etcd1 node 10.15.206.108 etcd2 node 10.15.206.109 etcd3 第一步：配置flannel网卡,先在etcd中注册flannel子网：1etcdctl set /coreos.com/network/config '&#123;"network": "172.16.0.0/16"&#125;' 第二步：在所有节点安装flannel1yum install -y flannel 第三步：修改flannel配置文件/etc/sysconfig/flanneld12FLANNEL_ETCD="http://10.15.206.107:2379,http://10.15.206.108:2379,http://10.15.206.109:2379" FLANNEL_ETCD_KEY="/coreos.com/network" 重启flannel：12systemctl start flanneld systemctl enable flanneld 需要说明的是，如果要让docker使用flannel的网络，docker必须要后于flannel启动，所以需要重新启动docker1systemctl restart docker 第四步：下载地址kubernetes-client地址https://storage.googleapis.com/kubernetes-release/release/v1.5.3/kubernetes-client-linux-amd64.tar.gz kubernetes-server地址：https://storage.googleapis.com/kubernetes-release/release/v1.5.3/kubernetes-server-linux-amd64.tar.gz 第五步：在server端服务器解压包tar zxvf kubernetes-server-linux-amd64.tar.gz cd kubernetes/server/bin 然后将文件复制到/usr/local/bin下1234for i in `ls -F|grep "*"|awk '&#123;print $1&#125;'|awk -F "*" '&#123;print $1&#125;'`;do cp $i /usr/local/bin/ ;done 第六步：启动master启动api-server12345678910kube-apiserver --address=0.0.0.0 --insecure-port=8080 --service-cluster-ip-range='10.15.206.120/24' --log_dir=/usr/local/kubernetes/logs/kube --kubelet_port=10250 --v=0 --logtostderr=false --etcd_servers=http://10.15.206.107:2379,http://10.15.206.108:2379,http://10.15.206.109:2379 --allow_privileged=false &gt;&gt; /usr/local/kubernetes/logs/kube-apiserver.log 2&gt;&amp;1 &amp; 启动controller-manager12345kube-controller-manager --v=0 --logtostderr=false --log_dir=/usr/local/kubernetes/logs/kube --master=10.15.206.120:8080 &gt;&gt; /usr/local/kubernetes/logs/kube-controller-manager 2&gt;&amp;1 &amp; 启动scheduler1234kube-scheduler --master='10.15.206.120:8080' --v=0 --log_dir=/usr/local/kubernetes/logs/kube &gt;&gt; /usr/local/kubernetes/logs/kube-scheduler.log 2&gt;&amp;1 &amp; 第七步：验证是否成功1234567kubectl get componentstatuses NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy &#123;"health": "true"&#125; etcd-0 Healthy &#123;"health": "true"&#125; etcd-1 Healthy &#123;"health": "true"&#125; 第八步：配置client123tar zxvf kubernetes-client-linux-amd64.tar.gz cd kubernetes/client/bin cp * /usr/local/bin/ 第九步：启动client启动kubelet123456789kubelet --logtostderr=false --v=0 --allow-privileged=false --log_dir=/usr/local/kubernetes/logs/kube --address=0.0.0.0 --port=10250 --hostname_override=10.15.206.120 --api_servers=http://10.15.206.120:8080 &gt;&gt; /usr/local/kubernetes/logs/kube-kubelet.log 2&gt;&amp;1 &amp; 启动proxy1234kube-proxy --logtostderr=false --v=0 --master=http://10.15.206.120 第三种安装方案(calico)环境介绍: 服务器 Ip Hosts Centos-7.3 10.200.102.95 k8s-master Centos-7.3 10.200.102.94 k8s-node-1 Centos-7.3 10.200.102.85 k8s-node-2 Centos-7.3 10.200.102.90 k8s-node-3 确保操作系统的内核是3.10版本以上的。并且关闭防火墙和selinux。123setenforce 0systemctl stop firewalld.servicesystemctl disable firewalld.service 根据需要是否配置必要的源，可以参考上述的源配置。 etc环境安装(可以选择集群的方案安装)服务器 IP Hosts| 服务器 | Ip | Hosts || ————- |:—————:|:———:|| Centos-7.3 | 10.200.102.85 | Echo0 || Centos-7.3 | 10.200.102.86 | Echo1 || Centos-7.3 | 10.200.102.84 | Echo2 | 安装ectd环境 配置etcd信息 启动服务所有的节点都进行如上相应的配置 安装k8s master环境1yum install kubernetes-master docker -y 配置好相应的kubernetes信息 配置好docker信息 查看集群信息 安装k8s node环境1yum install kubernetes-node docker –y 配置k8s和docker信息配置kubectl配置proxy配置config配置docker镜像拉取位置 查看集群信息 安装kube-dns环境(master节点)1234567891011下载kube-dns命令# wget https://dl.k8s.io/v1.5.2/kubernetes-server-linux-amd64.tar.gz# tar -xf kubernetes-server-linux-amd64.tar.gz# mv /opt/docker/src/kubernetes/server/bin/kube-dns /usr/bin/新建kube-dns配置文件# vi /etc/kubernetes/kube-dnsKUBE_DNS_PORT="--dns-port=53"KUBE_DNS_DOMAIN="--domain=cluster.local"KUBE_DNS_MASTER=--kube-master-url="http://10.200.102.95:8080”KUBE_DNS_ARGS="" 12345678910111213141516171819新建kube-dns.service配置文件# cat /usr/lib/systemd/system/kube-dns.service[Unit]Description=Kubernetes Kube-dns ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=kube-apiserver.serviceRequires=kube-apiserver.service [Service]WorkingDirectory=/var/lib/kube-dnsEnvironmentFile=-/etc/kubernetes/kube-dnsExecStart=/usr/bin/kube-dns \ $KUBE_DNS_PORT \ $KUBE_DNS_DOMAIN \ $KUBE_DNS_MASTER \ $KUBE_DNS_ARGSRestart=on-failure[Install]WantedBy=multi-user.target 123456789101112Master启动# mkdir -p /var/lib/kube-dns# systemctl enable kube-dns# systemctl restart kube-dnsmaster修改/etc/resolv.conf文件# cat /etc/resolv.conf# Generated by NetworkManagersearch default.svc.cluster.local svc.cluster.local cluster.localnameserver 10.200.102.95nameserver 223.5.5.5nameserver 202.96.128.86 node结点修改kubelet文件 验证kube-dns是否安装成功 安装calico环境配置各个节点docker环境： 配置好，记得重启docker12# systemctl daemon-reload# systemctl restart docker 下载calico插件1234567891011121314151617181920Master节点：# wget https://github.com/projectcalico/calicoctl/releases/download/v1.1.0/calicoctl# chmod +x calicoctl# mv calicoctl /usr/bin/# docker pull docker.io/calico/node:v1.1.0# docker tag docker.io/calico/node:v1.1.0 quay.io/calico/node:v1.1.0#wget N -P /opt/cni/bin/ https://github.com/projectcalico/calico-cni/releases/download/v1.6.0/calico# wget -N -P /opt/cni/bin/ https://github.com/projectcalico/calico-cni/releases/download/v1.6.0/calico-ipam# chmod +x /opt/cni/bin/calico /opt/cni/bin/calico-ipamNode节点：# docker pull docker.io/calico/node:v1.1.0# docker tag docker.io/calico/node:v1.1.0 quay.io/calico/node:v1.1.0# wget -N -P /opt/cni/bin/ https://github.com/projectcalico/calico-cni/releases/download/v1.6.0/calico# wget -N -P /opt/cni/bin/ https://github.com/projectcalico/calico-cni/releases/download/v1.6.0/calico-ipam# chmod +x /opt/cni/bin/calico /opt/cni/bin/calico-ipam 配置文件(所有节点) Master机上wget http://docs.projectcalico.org/v1.5/getting-started/kubernetes/installation/policy-controller.yaml 修改 policy-controller.yaml文件里的etcd的地址 启动文件：1234567891011# serivce etcd restart# kubectl create -f policy-controller.yaml每个节点上启动calico-node服务(ETCD_AUTHORITY可以配置多个（集群方案）)# systemctl enable calico-node# systemctl start calico-node# export ETCD_AUTHORITY=10.200.102.85:2379验证calico是否启动正常calicoctl node statuscalicoctl get nodes --out=wide 添加子网 至此calico的k8s方案搭建成功]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java HashMap 源码解析]]></title>
      <url>%2F2017%2F04%2F19%2FJava-HashMap-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%2F</url>
      <content type="text"><![CDATA[优秀的人，不是不合群，而是他们合群的人里面没有你继上一篇文章Java集合框架综述后，今天正式开始分析具体集合类的代码，首先以既熟悉又陌生的HashMap开始。 签名(signature)123public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable 可以看到HashMap继承了 标记接口Cloneable，用于表明HashMap对象会重写java.lang.Object#clone()方法，HashMap实现的是浅拷贝（shallow copy）。 标记接口Serializable，用于表明HashMap对象可以被序列化比较有意思的是，HashMap同时继承了抽象类AbstractMap与接口Map，因为抽象类AbstractMap的签名为123public abstract class AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt; Stack Overfloooow上解释到：在语法层面继承接口Map是多余的，这么做仅仅是为了让阅读代码的人明确知道HashMap是属于Map体系的，起到了文档的作用 AbstractMap相当于个辅助类，Map的一些操作这里面已经提供了默认实现，后面具体的子类如果没有特殊行为，可直接使用AbstractMap提供的实现。 Cloneable接口Cloneable这个接口设计的非常不好，最致命的一点是它里面竟然没有clone方法，也就是说我们自己写的类完全可以实现这个接口的同时不重写clone方法。关于Cloneable的不足，大家可以去看看《Effective Java》一书的作者给出的理由，在所给链接的文章里，Josh Bloch也会讲如何实现深拷贝比较好，我这里就不在赘述了。 Map接口在eclipse中的outline面板可以看到Map接口里面包含以下成员方法与内部类： Map_field_method可以看到，这里的成员方法不外乎是“增删改查”，这也反映了我们编写程序时，一定是以“数据”为导向的。 在上篇文章讲了Map虽然并不是Collection，但是它提供了三种“集合视角”(collection views)，与下面三个方法一一对应： Set keySet()，提供key的集合视角 Collection values()，提供value的集合视角 Set]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java集合框架综述]]></title>
      <url>%2F2017%2F04%2F19%2FJava%E9%9B%86%E5%90%88%E6%A1%86%E6%9E%B6%E7%BB%BC%E8%BF%B0%2F</url>
      <content type="text"><![CDATA[优秀的人，不是不合群，而是他们合群的人里面没有你最近被陆陆续续问了几遍HashMap的实现，回答的不好，打算复习复习JDK中的集合框架，并尝试分析其源码，这么做一方面是这些类非常实用，掌握其实现能更好的优化我们的程序；另一方面是学习借鉴JDK是如何实现了这么一套优雅高效的类库，提升编程能力。在介绍具体适合类之前，本篇文章对Java中的集合框架做一个大致描述，从一个高的角度俯视这个框架，了解了这个框架的一些理念与约定，会大大帮助后面分析某个具体类，让我们开始吧。 集合框架(collections framework)首先要明确，集合代表了一组对象(和数组一样，但数组长度不能变，而集合能)。Java中的集合框架定义了一套规范，用来表示、操作集合，使具体操作与实现细节解耦。其实说白了，可以把一个集合看成一个微型数据库，操作不外乎“增删改查”四种操作，我们在学习使用一个具体的集合类时，需要把这四个操作的时空复杂度弄清楚了，基本上就可以说掌握这个类了。 设计理念主要理念用一句话概括就是：提供一套“小而美”的API。API需要对程序员友好，增加新功能时能让程序员们快速上手。为了保证核心接口足够小，最顶层的接口(也就是Collection与Map接口)并不会区分该集合是否可变(mutability),是否可更改(modifiability),是否可改变大小(resizability)这些细微的差别。相反，一些操作是可选的，在实现时抛出UnsupportedOperationException即可表示集合不支持该操作。集合的实现者必须在文档中声明那些操作是不支持的。为了保证最顶层的核心接口足够小，它们只能包含下面情况下的方法： 基本操作，像之前说的“增删改查” There is a compelling performance reason why an important implementation would want to override it. 此外，所有的集合类都必须能提供友好的交互操作，这包括没有继承Collection类的数组对象。因此，框架提供一套方法，让集合类与数组可以相互转化，并且可以把Map看作成集合。 两大基类Collection与Map在集合框架的类继承体系中，最顶层有两个接口: Collection表示一组纯数据 Map表示一组key-value对 一般继承自Collection或Map的集合类，会提供两个“标准”的构造函数: 没有参数的构造函数，创建一个空的集合类 有一个类型与基类（Collection或Map）相同的构造函数，创建一个与给定参数具有相同元素的新集合类 因为接口中不能包含构造函数，所以上面这两个构造函数的约定并不是强制性的，但是在目前的集合框架中，所有继承自Collection或Map的子类都遵循这一约定。 Collection java-collection-hierarchy 如上图所示，Collection类主要有三个接口： Set表示不允许有重复元素的集合(A collection that contains no duplicate elements) List表示允许有重复元素的集合(An ordered collection (also known as a sequence)) Queue JDK1.5新增，与上面两个集合类主要是的区分在于Queue主要用于存储数据，而不是处理数据。(A collection designed for holding elements prior to processing.) Map MapClassHierarchy Map并不是一个真正意义上的集合(are not true collections)，但是这个接口提供了三种“集合视角”(collection views )，使得可以像操作集合一样操作它们，具体如下： 把map的内容看作key的集合(map’s contents to be viewed as a set of keys) 把map的内容看作value的集合(map’s contents to be viewed as a collection of values) 把map的内容看作key-value映射的集合(map’s contents to be viewed as a set of key-value mappings) 集合的实现(Collection Implementations)实现集合接口的类一般遵循&lt;实现方式&gt;+&lt;接口&gt;的命名方式，通用的集合实现类如下表： Interface Hash Table Resizable Array Balanced Tree Linked List Hash Table + Linked List Set HashSet TreeSet LinkedList List ArrayList LinkedList Deque ArrayDeque Map HashMap TreeMap LinkedHashMap 总结今天先开个头，后面会陆陆续续来一系列干货，Stay Tuned。需要说明一点，今后所有源码分析都将基于Oracle JDK 1.7.0_71，请知悉。 1234$ java -versionjava version"1.7.0_71"Java(TM) SE Runtime Environment (build 1.7.0_71-b14)Java HotSpot(TM) 64-Bit Server VM (build 24.71-b01, mixed mode) 参考 http://docs.oracle.com/javase/7/docs/technotes/guides/collections/overview.html https://en.wikipedia.org/wiki/Java_collections_framework]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>%2F2017%2F04%2F18%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
    </entry>

    
  
  
</search>
