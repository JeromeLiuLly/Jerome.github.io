<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[Mycat HA(高可用) 与 LB(负载均衡)]]></title>
      <url>%2F2017%2F06%2F16%2FMycat-HA-%E9%AB%98%E5%8F%AF%E7%94%A8-%E4%B8%8E-LB-%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%2F</url>
      <content type="text"><![CDATA[优秀的人，不是不合群，而是他们合群的人里面没有你 前言Mycat 是一款开源的数据库分库分表中间件一般而言在生产环境下，任何基础架构都有必要考虑 高可用,扩展性和监控方案 : 监控：使用 Mycat web (也有叫 Mycat-eye )可以实现对mycat的 监控 高可用：Mycat 目前没有官方的高可用解决方案 ，但配合使用 keepalived 和 haproxy 也可以实现mycat的 高可用 可扩展：由于 Mycat 本身是无状态的，可以通过添加 Mycat 节点来实现 水平扩展 ，从而分摊访问压力 下面分享一下 Mycat 高可用与负载均衡 的实现方法，详细内容可以参考 官方文档 (但是由于官方文档比较老，有不少坑，这篇分享里会将这些坑填平) Tip: 当前的最新版本为 Mycat server 1.5 GA 、 Keepalived for Linux - Version 1.2.19 、 HAProxy 1.6.3 概要Mycat 高可用架构Mycat 是无状态的，可以使用 HAProxy 或四层交换机等设备构建 Mycat 高可用集群，后端 Mysql 则配置为主从同步 (replication 、mha 或 galary 集群)，那么Mycat层和Mysql层就都是高可用的，整个分布式数据库系统就是高可用的没有四层交换机的环境下，为了实现系统架构的高扩展性，可以使用 LVS 或 HAProxy 替代 (开源软件最显著的好处就是便宜) ，不过引入了四层(TCP层)交换逻辑或服务后，又会增加此层的单点风险，为了有效规避，可以使用 keepalived 来进行故障转移(故障判断和服务漂移)这样，HAProxy 层和 Mycat 层任意宕机一半，整个系统依旧可以正常运转 由于对业务层透明，个别请求在实际IP切换的过程中可能会有一次timeout，但自动重发请求就能恢复正常，一般应用也都有重发机制 下载安装keepalivedkeepalived 项目是为了结合 LVS 在Linux平台上构建简单而健壮的高可用负载均衡系统而产生的，不过 keepalived 也可以单独出来作为构建高可用系统的基础服务，对 浮动IP (VIP，也有叫服务IP)进行管理 keepalived 的 下载地址 下载++1234567891011121314[root@h101 keepalived]# wget http://www.keepalived.org/software/keepalived-1.2.19.tar.gz--2016-03-02 15:26:58-- http://www.keepalived.org/software/keepalived-1.2.19.tar.gzResolving www.keepalived.org... 37.59.63.157, 2001:41d0:8:7a9d::1Connecting to www.keepalived.org|37.59.63.157|:80... connected.HTTP request sent, awaiting response... 200 OKLength: 330164 (322K) [application/x-gzip]Saving to: “keepalived-1.2.19.tar.gz”100%[============================================================================&gt;] 330,164 35.7K/s in 10s 2016-03-02 15:27:11 (32.1 KB/s) - “keepalived-1.2.19.tar.gz” saved [330164/330164][root@h101 keepalived]# 解压++123456789101112131415161718[root@h101 keepalived]# tar -zxvf keepalived-1.2.19.tar.gz keepalived-1.2.19/keepalived-1.2.19/keepalived.spec.inkeepalived-1.2.19/Makefile.inkeepalived-1.2.19/genhash/......keepalived-1.2.19/lib/vector.ckeepalived-1.2.19/lib/scheduler.hkeepalived-1.2.19/lib/signals.ckeepalived-1.2.19/TODO[root@h101 keepalived]# lskeepalived-1.2.19 keepalived-1.2.19.tar.gz[root@h101 keepalived]# cd keepalived-1.2.19[root@h101 keepalived-1.2.19]# lsAUTHOR ChangeLog configure.in COPYING genhash install-sh keepalived.spec.in Makefile.in TODObin configure CONTRIBUTORS doc INSTALL keepalived lib README VERSION[root@h101 keepalived-1.2.19]# 安装使用下列命令进行安装++12./configure --prefix=/usr/local/keepalivedmake &amp;&amp; make install Tip: 可以使用 echo $? 来确认执行结果 详细安装过程++123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226[root@h101 keepalived-1.2.19]# lsAUTHOR ChangeLog configure.in COPYING genhash install-sh keepalived.spec.in Makefile.in TODObin configure CONTRIBUTORS doc INSTALL keepalived lib README VERSION[root@h101 keepalived-1.2.19]# ./configure --prefix=/usr/local/keepalived checking for gcc... gccchecking whether the C compiler works... yeschecking for C compiler default output file name... a.outchecking for suffix of executables... checking whether we are cross compiling... nochecking for suffix of object files... ochecking whether we are using the GNU C compiler... yeschecking whether gcc accepts -g... yeschecking for gcc option to accept ISO C89... none neededchecking for a BSD-compatible install... /usr/bin/install -cchecking for strip... stripchecking how to run the C preprocessor... gcc -Echecking for grep that handles long lines and -e... /bin/grepchecking for egrep... /bin/grep -Echecking for ANSI C header files... yeschecking for sys/wait.h that is POSIX.1 compatible... yeschecking for sys/types.h... yeschecking for sys/stat.h... yeschecking for stdlib.h... yeschecking for string.h... yeschecking for memory.h... yeschecking for strings.h... yeschecking for inttypes.h... yeschecking for stdint.h... yeschecking for unistd.h... yeschecking fcntl.h usability... yeschecking fcntl.h presence... yeschecking for fcntl.h... yeschecking syslog.h usability... yeschecking syslog.h presence... yeschecking for syslog.h... yeschecking for unistd.h... (cached) yeschecking sys/ioctl.h usability... yeschecking sys/ioctl.h presence... yeschecking for sys/ioctl.h... yeschecking sys/time.h usability... yeschecking sys/time.h presence... yeschecking for sys/time.h... yeschecking openssl/ssl.h usability... yeschecking openssl/ssl.h presence... yeschecking for openssl/ssl.h... yeschecking openssl/md5.h usability... yeschecking openssl/md5.h presence... yeschecking for openssl/md5.h... yeschecking openssl/err.h usability... yeschecking openssl/err.h presence... yeschecking for openssl/err.h... yeschecking whether ETHERTYPE_IPV6 is declared... yeschecking for crypt in -lcrypt... yeschecking for MD5_Init in -lcrypto... yeschecking for SSL_CTX_new in -lssl... yeschecking for nl_socket_alloc in -lnl-3... nochecking for nl_socket_modify_cb in -lnl... noconfigure: WARNING: keepalived will be built without libnl support.checking for kernel version... 2.6.32checking for IPVS syncd support... yeschecking for kernel macvlan support... yeschecking whether SO_MARK is declared... yeschecking for an ANSI C-conforming const... yeschecking for pid_t... yeschecking whether time.h and sys/time.h may both be included... yeschecking whether gcc needs -traditional... nochecking for working memcmp... yeschecking return type of signal handlers... voidchecking for gettimeofday... yeschecking for select... yeschecking for socket... yeschecking for strerror... yeschecking for strtol... yeschecking for uname... yesconfigure: creating ./config.statusconfig.status: creating Makefileconfig.status: creating genhash/Makefileconfig.status: creating keepalived/core/Makefileconfig.status: creating lib/config.hconfig.status: creating keepalived.specconfig.status: creating keepalived/Makefileconfig.status: creating lib/Makefileconfig.status: creating keepalived/vrrp/Makefileconfig.status: creating keepalived/check/Makefileconfig.status: creating keepalived/libipvs-2.6/MakefileKeepalived configuration------------------------Keepalived version : 1.2.19Compiler : gccCompiler flags : -g -O2Extra Lib : -lssl -lcrypto -lcrypt Use IPVS Framework : YesIPVS sync daemon support : YesIPVS use libnl : Nofwmark socket support : YesUse VRRP Framework : YesUse VRRP VMAC : YesSNMP support : NoSHA1 support : NoUse Debug flags : No[root@h101 keepalived-1.2.19]# echo $?0[root@h101 keepalived-1.2.19]# make make -C lib || exit 1;make[1]: Entering directory `/usr/local/src/keepalived/keepalived-1.2.19/lib'gcc -I. -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_WITHOUT_SNMP_ -c memory.cgcc -I. -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_WITHOUT_SNMP_ -c utils.cgcc -I. -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_WITHOUT_SNMP_ -c notify.cgcc -I. -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_WITHOUT_SNMP_ -c timer.cgcc -I. -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_WITHOUT_SNMP_ -c scheduler.cgcc -I. -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_WITHOUT_SNMP_ -c vector.cgcc -I. -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_WITHOUT_SNMP_ -c list.cgcc -I. -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_WITHOUT_SNMP_ -c html.cgcc -I. -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_WITHOUT_SNMP_ -c parser.cgcc -I. -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_WITHOUT_SNMP_ -c signals.cgcc -I. -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_WITHOUT_SNMP_ -c logger.cmake[1]: Leaving directory `/usr/local/src/keepalived/keepalived-1.2.19/lib'make -C keepalivedmake[1]: Entering directory `/usr/local/src/keepalived/keepalived-1.2.19/keepalived'make[2]: Entering directory `/usr/local/src/keepalived/keepalived-1.2.19/keepalived/core'gcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_WITH_VRRP_ -D_WITHOUT_SNMP_ -D_WITH_SO_MARK_ -c main.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_WITH_VRRP_ -D_WITHOUT_SNMP_ -D_WITH_SO_MARK_ -c daemon.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_WITH_VRRP_ -D_WITHOUT_SNMP_ -D_WITH_SO_MARK_ -c pidfile.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_WITH_VRRP_ -D_WITHOUT_SNMP_ -D_WITH_SO_MARK_ -c layer4.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_WITH_VRRP_ -D_WITHOUT_SNMP_ -D_WITH_SO_MARK_ -c smtp.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_WITH_VRRP_ -D_WITHOUT_SNMP_ -D_WITH_SO_MARK_ -c global_data.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_WITH_VRRP_ -D_WITHOUT_SNMP_ -D_WITH_SO_MARK_ -c global_parser.cmake[2]: Leaving directory `/usr/local/src/keepalived/keepalived-1.2.19/keepalived/core'make[2]: Entering directory `/usr/local/src/keepalived/keepalived-1.2.19/keepalived/check'gcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_WITH_VRRP_ -D_WITHOUT_SNMP_ -D_WITH_SO_MARK_ -c check_daemon.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_WITH_VRRP_ -D_WITHOUT_SNMP_ -D_WITH_SO_MARK_ -c check_data.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_WITH_VRRP_ -D_WITHOUT_SNMP_ -D_WITH_SO_MARK_ -c check_parser.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_WITH_VRRP_ -D_WITHOUT_SNMP_ -D_WITH_SO_MARK_ -c check_api.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_WITH_VRRP_ -D_WITHOUT_SNMP_ -D_WITH_SO_MARK_ -c check_tcp.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_WITH_VRRP_ -D_WITHOUT_SNMP_ -D_WITH_SO_MARK_ -c check_http.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_WITH_VRRP_ -D_WITHOUT_SNMP_ -D_WITH_SO_MARK_ -c check_ssl.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_WITH_VRRP_ -D_WITHOUT_SNMP_ -D_WITH_SO_MARK_ -c check_smtp.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_WITH_VRRP_ -D_WITHOUT_SNMP_ -D_WITH_SO_MARK_ -c check_misc.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_WITH_VRRP_ -D_WITHOUT_SNMP_ -D_WITH_SO_MARK_ -c ipwrapper.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_WITH_VRRP_ -D_WITHOUT_SNMP_ -D_WITH_SO_MARK_ -c ipvswrapper.cmake[2]: Leaving directory `/usr/local/src/keepalived/keepalived-1.2.19/keepalived/check'make[2]: Entering directory `/usr/local/src/keepalived/keepalived-1.2.19/keepalived/vrrp'gcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_HAVE_VRRP_VMAC_ -D_WITHOUT_SNMP_ -c vrrp_daemon.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_HAVE_VRRP_VMAC_ -D_WITHOUT_SNMP_ -c vrrp_print.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_HAVE_VRRP_VMAC_ -D_WITHOUT_SNMP_ -c vrrp_data.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_HAVE_VRRP_VMAC_ -D_WITHOUT_SNMP_ -c vrrp_parser.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_HAVE_VRRP_VMAC_ -D_WITHOUT_SNMP_ -c vrrp.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_HAVE_VRRP_VMAC_ -D_WITHOUT_SNMP_ -c vrrp_notify.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_HAVE_VRRP_VMAC_ -D_WITHOUT_SNMP_ -c vrrp_scheduler.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_HAVE_VRRP_VMAC_ -D_WITHOUT_SNMP_ -c vrrp_sync.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_HAVE_VRRP_VMAC_ -D_WITHOUT_SNMP_ -c vrrp_index.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_HAVE_VRRP_VMAC_ -D_WITHOUT_SNMP_ -c vrrp_netlink.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_HAVE_VRRP_VMAC_ -D_WITHOUT_SNMP_ -c vrrp_arp.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_HAVE_VRRP_VMAC_ -D_WITHOUT_SNMP_ -c vrrp_if.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_HAVE_VRRP_VMAC_ -D_WITHOUT_SNMP_ -c vrrp_track.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_HAVE_VRRP_VMAC_ -D_WITHOUT_SNMP_ -c vrrp_ipaddress.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_HAVE_VRRP_VMAC_ -D_WITHOUT_SNMP_ -c vrrp_iproute.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_HAVE_VRRP_VMAC_ -D_WITHOUT_SNMP_ -c vrrp_ipsecah.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_HAVE_VRRP_VMAC_ -D_WITHOUT_SNMP_ -c vrrp_ndisc.cgcc -I../include -I../../lib -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -D_KRNL_2_6_ -D_WITH_LVS_ -D_HAVE_IPVS_SYNCD_ -D_HAVE_VRRP_VMAC_ -D_WITHOUT_SNMP_ -c vrrp_vmac.cmake[2]: Leaving directory `/usr/local/src/keepalived/keepalived-1.2.19/keepalived/vrrp'make[2]: Entering directory `/usr/local/src/keepalived/keepalived-1.2.19/keepalived/libipvs-2.6'gcc -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -DLIBIPVS_DONTUSE_NL -Wall -Wunused -c -o libipvs.o libipvs.cgcc -g -O2 -I/usr/src/linux/include -I/usr/src/linux/include -DLIBIPVS_DONTUSE_NL -Wall -Wunused -c -o ip_vs_nl_policy.o ip_vs_nl_policy.car rv libipvs.a libipvs.o ip_vs_nl_policy.oar: creating libipvs.aa - libipvs.oa - ip_vs_nl_policy.orm libipvs.o ip_vs_nl_policy.omake[2]: Leaving directory `/usr/local/src/keepalived/keepalived-1.2.19/keepalived/libipvs-2.6'Building ../bin/keepalivedstrip ../bin/keepalivedMake completemake[1]: Leaving directory `/usr/local/src/keepalived/keepalived-1.2.19/keepalived'make -C genhashmake[1]: Entering directory `/usr/local/src/keepalived/keepalived-1.2.19/genhash'gcc -I../lib -g -O2 -D_WITH_SO_MARK_ -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -c -o main.o main.cgcc -I../lib -g -O2 -D_WITH_SO_MARK_ -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -c -o sock.o sock.cgcc -I../lib -g -O2 -D_WITH_SO_MARK_ -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -c -o layer4.o layer4.cgcc -I../lib -g -O2 -D_WITH_SO_MARK_ -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -c -o http.o http.cgcc -I../lib -g -O2 -D_WITH_SO_MARK_ -I/usr/src/linux/include -I/usr/src/linux/include -Wall -Wunused -Wstrict-prototypes -c -o ssl.o ssl.cBuilding ../bin/genhashstrip ../bin/genhashMake completemake[1]: Leaving directory `/usr/local/src/keepalived/keepalived-1.2.19/genhash'Make complete[root@h101 keepalived-1.2.19]# echo $?0[root@h101 keepalived-1.2.19]# make install make -C keepalived installmake[1]: Entering directory `/usr/local/src/keepalived/keepalived-1.2.19/keepalived'install -d /usr/local/keepalived/sbininstall -m 700 ../bin/keepalived /usr/local/keepalived/sbin/install -d /usr/local/keepalived/etc/rc.d/init.dinstall -m 755 etc/init.d/keepalived.init /usr/local/keepalived/etc/rc.d/init.d/keepalivedinstall -d /usr/local/keepalived/etc/sysconfiginstall -m 644 etc/init.d/keepalived.sysconfig /usr/local/keepalived/etc/sysconfig/keepalivedinstall -d /usr/local/keepalived/etc/keepalived/samplesinstall -m 644 etc/keepalived/keepalived.conf /usr/local/keepalived/etc/keepalived/install -m 644 ../doc/samples/* /usr/local/keepalived/etc/keepalived/samples/install -d /usr/local/keepalived/share/man/man5install -d /usr/local/keepalived/share/man/man8install -m 644 ../doc/man/man5/keepalived.conf.5 /usr/local/keepalived/share/man/man5install -m 644 ../doc/man/man8/keepalived.8 /usr/local/keepalived/share/man/man8make[1]: Leaving directory `/usr/local/src/keepalived/keepalived-1.2.19/keepalived'make -C genhash installmake[1]: Entering directory `/usr/local/src/keepalived/keepalived-1.2.19/genhash'install -d /usr/local/keepalived/bininstall -m 755 ../bin/genhash /usr/local/keepalived/bin/install -d /usr/local/keepalived/share/man/man1install -m 644 ../doc/man/man1/genhash.1 /usr/local/keepalived/share/man/man1make[1]: Leaving directory `/usr/local/src/keepalived/keepalived-1.2.19/genhash'[root@h101 keepalived-1.2.19]# echo $?0[root@h101 keepalived-1.2.19]# ll /usr/local/keepalived/total 16drwxr-xr-x 2 root root 4096 Mar 2 15:58 bindrwxr-xr-x 5 root root 4096 Mar 2 15:58 etcdrwxr-xr-x 2 root root 4096 Mar 2 15:58 sbindrwxr-xr-x 3 root root 4096 Mar 2 15:58 share[root@h101 keepalived-1.2.19]# 目录结构++12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[root@h101 sbin]# tree /usr/local/keepalived//usr/local/keepalived/├── bin│ └── genhash├── etc│ ├── keepalived│ │ ├── keepalived.conf│ │ └── samples│ │ ├── client.pem│ │ ├── dh1024.pem│ │ ├── keepalived.conf.fwmark│ │ ├── keepalived.conf.HTTP_GET.port│ │ ├── keepalived.conf.inhibit│ │ ├── keepalived.conf.IPv6│ │ ├── keepalived.conf.misc_check│ │ ├── keepalived.conf.misc_check_arg│ │ ├── keepalived.conf.quorum│ │ ├── keepalived.conf.sample│ │ ├── keepalived.conf.SMTP_CHECK│ │ ├── keepalived.conf.SSL_GET│ │ ├── keepalived.conf.status_code│ │ ├── keepalived.conf.track_interface│ │ ├── keepalived.conf.virtualhost│ │ ├── keepalived.conf.virtual_server_group│ │ ├── keepalived.conf.vrrp│ │ ├── keepalived.conf.vrrp.localcheck│ │ ├── keepalived.conf.vrrp.lvs_syncd│ │ ├── keepalived.conf.vrrp.routes│ │ ├── keepalived.conf.vrrp.scripts│ │ ├── keepalived.conf.vrrp.static_ipaddress│ │ ├── keepalived.conf.vrrp.sync│ │ ├── root.pem│ │ └── sample.misccheck.smbcheck.sh│ ├── rc.d│ │ └── init.d│ │ └── keepalived│ └── sysconfig│ └── keepalived├── sbin│ └── keepalived└── share └── man ├── man1 │ └── genhash.1 ├── man5 │ └── keepalived.conf.5 └── man8 └── keepalived.813 directories, 33 files[root@h101 sbin]# 版本确认++123[root@h101 sbin]# /usr/local/keepalived/sbin/keepalived -v Keepalived v1.2.19 (03/02,2016)[root@h101 sbin]# 下载安装HAProxyHAProxy 是一个稳定且开源的高性能 TCP/HTTP 负载均衡代理软件 HAProxy 的 下载地址 Tip: 访问 HAProxy 和 下载地址，可能会被墙，所以准备好VPN或代理 解压++123456789101112131415161718192021222324252627282930313233343536373839404142[root@h101 haproxy]# lshaproxy-1.6.3.tar.gz[root@h101 haproxy]# md5sum haproxy-1.6.3.tar.gz 3362d1e268c78155c2474cb73e7f03f9 haproxy-1.6.3.tar.gz[root@h101 haproxy]# tar -zxvf haproxy-1.6.3.tar.gz haproxy-1.6.3/haproxy-1.6.3/.gitignorehaproxy-1.6.3/CHANGELOGhaproxy-1.6.3/CONTRIBUTINGhaproxy-1.6.3/LICENSEhaproxy-1.6.3/MAINTAINERShaproxy-1.6.3/Makefilehaproxy-1.6.3/README......haproxy-1.6.3/tests/test.chaproxy-1.6.3/tests/test_hashes.chaproxy-1.6.3/tests/test_pools.chaproxy-1.6.3/tests/testinet.chaproxy-1.6.3/tests/uri_hash.c[root@h101 haproxy]# lshaproxy-1.6.3 haproxy-1.6.3.tar.gz[root@h101 haproxy]# ll haproxy-1.6.3total 488-rw-rw-r-- 1 root root 351146 Dec 27 22:04 CHANGELOGdrwxrwxr-x 10 root root 4096 Dec 27 22:04 contrib-rw-rw-r-- 1 root root 36238 Dec 27 22:04 CONTRIBUTINGdrwxrwxr-x 5 root root 4096 Dec 27 22:04 docdrwxrwxr-x 2 root root 4096 Dec 27 22:04 ebtreedrwxrwxr-x 3 root root 4096 Dec 27 22:04 examplesdrwxrwxr-x 6 root root 4096 Dec 27 22:04 include-rw-rw-r-- 1 root root 2029 Dec 27 22:04 LICENSE-rw-rw-r-- 1 root root 2216 Dec 27 22:04 MAINTAINERS-rw-rw-r-- 1 root root 31547 Dec 27 22:04 Makefile-rw-rw-r-- 1 root root 22419 Dec 27 22:04 README-rw-rw-r-- 1 root root 3305 Dec 27 22:04 ROADMAPdrwxrwxr-x 2 root root 4096 Dec 27 22:04 src-rw-rw-r-- 1 root root 14 Dec 27 22:04 SUBVERSdrwxrwxr-x 2 root root 4096 Dec 27 22:04 tests-rw-rw-r-- 1 root root 24 Dec 27 22:04 VERDATE-rw-rw-r-- 1 root root 6 Dec 27 22:04 VERSION[root@h101 haproxy]# 编译安装使用下面命令进行编译安装++12make TARGET=linux2628 ARCH=x86_64 PREFIX=/usr/local/haproxymake install PREFIX=/usr/local/haproxy 详细安装过程++12345678910111213141516171819202122232425262728293031323334[root@h101 haproxy]# cd haproxy-1.6.3[root@h101 haproxy-1.6.3]# lsCHANGELOG CONTRIBUTING ebtree include MAINTAINERS README src tests VERSIONcontrib doc examples LICENSE Makefile ROADMAP SUBVERS VERDATE[root@h101 haproxy-1.6.3]# make TARGET=linux2628 ARCH=x86_64 PREFIX=/usr/local/haproxygcc -Iinclude -Iebtree -Wall -m64 -march=x86-64 -O2 -g -fno-strict-aliasing -Wdeclaration-after-statement -DCONFIG_HAP_LINUX_SPLICE -DTPROXY -DCONFIG_HAP_LINUX_TPROXY -DCONFIG_HAP_CRYPT -DENABLE_POLL -DENABLE_EPOLL -DUSE_CPU_AFFINITY -DASSUME_SPLICE_WORKS -DUSE_ACCEPT4 -DNETFILTER -DUSE_GETSOCKNAME -DCONFIG_HAPROXY_VERSION=\"1.6.3\" -DCONFIG_HAPROXY_DATE=\"2015/12/25\" \ -DBUILD_TARGET='"linux2628"' \ -DBUILD_ARCH='"x86_64"' \ -DBUILD_CPU='"generic"' \ -DBUILD_CC='"gcc"' \ -DBUILD_CFLAGS='"-m64 -march=x86-64 -O2 -g -fno-strict-aliasing -Wdeclaration-after-statement"' \ -DBUILD_OPTIONS='""' \ -c -o src/haproxy.o src/haproxy.cgcc -Iinclude -Iebtree -Wall -m64 -march=x86-64 -O2 -g -fno-strict-aliasing -Wdeclaration-after-statement -DCONFIG_HAP_LINUX_SPLICE -DTPROXY -DCONFIG_HAP_LINUX_TPROXY -DCONFIG_HAP_CRYPT -DENABLE_POLL -DENABLE_EPOLL -DUSE_CPU_AFFINITY -DASSUME_SPLICE_WORKS -DUSE_ACCEPT4 -DNETFILTER -DUSE_GETSOCKNAME -DCONFIG_HAPROXY_VERSION=\"1.6.3\" -DCONFIG_HAPROXY_DATE=\"2015/12/25\" -c -o src/base64.o src/base64.c......gcc -Iinclude -Iebtree -Wall -m64 -march=x86-64 -O2 -g -fno-strict-aliasing -Wdeclaration-after-statement -DCONFIG_HAP_LINUX_SPLICE -DTPROXY -DCONFIG_HAP_LINUX_TPROXY -DCONFIG_HAP_CRYPT -DENABLE_POLL -DENABLE_EPOLL -DUSE_CPU_AFFINITY -DASSUME_SPLICE_WORKS -DUSE_ACCEPT4 -DNETFILTER -DUSE_GETSOCKNAME -DCONFIG_HAPROXY_VERSION=\"1.6.3\" -DCONFIG_HAPROXY_DATE=\"2015/12/25\" \ -DSBINDIR='"/usr/local/haproxy/sbin"' \ -c -o src/haproxy-systemd-wrapper.o src/haproxy-systemd-wrapper.cgcc -m64 -march=x86-64 -g -o haproxy-systemd-wrapper src/haproxy-systemd-wrapper.o -lcrypt -ldl [root@h101 haproxy-1.6.3]# echo $?0[root@h101 haproxy-1.6.3]# make install PREFIX=/usr/local/haproxyinstall -d "/usr/local/haproxy/sbin"install haproxy "/usr/local/haproxy/sbin"install -d "/usr/local/haproxy/share/man"/man1install -m 644 doc/haproxy.1 "/usr/local/haproxy/share/man"/man1install -d "/usr/local/haproxy/doc/haproxy"for x in architecture close-options configuration cookie-options intro linux-syn-cookies lua management network-namespaces proxy-protocol; do \ install -m 644 doc/$x.txt "/usr/local/haproxy/doc/haproxy" ; \ done[root@h101 haproxy-1.6.3]# echo $?0[root@h101 haproxy-1.6.3]# 目录结构++12345678910111213141516171819202122232425262728[root@h101 haproxy-1.6.3]# ll /usr/local/haproxy/total 12drwxr-xr-x 3 root root 4096 Mar 2 16:23 docdrwxr-xr-x 2 root root 4096 Mar 2 16:23 sbindrwxr-xr-x 3 root root 4096 Mar 2 16:23 share[root@h101 haproxy-1.6.3]# tree /usr/local/haproxy//usr/local/haproxy/├── doc│ └── haproxy│ ├── architecture.txt│ ├── close-options.txt│ ├── configuration.txt│ ├── cookie-options.txt│ ├── intro.txt│ ├── linux-syn-cookies.txt│ ├── lua.txt│ ├── management.txt│ ├── network-namespaces.txt│ └── proxy-protocol.txt├── sbin│ └── haproxy└── share └── man └── man1 └── haproxy.16 directories, 12 files[root@h101 haproxy-1.6.3]# 版本确认++12345[root@h101 ~]# /usr/local/haproxy/sbin/haproxy -vHA-Proxy version 1.6.3 2015/12/25Copyright 2000-2015 Willy Tarreau &lt;willy@haproxy.org&gt;[root@h101 ~]# 配置rsyslog日志日志是可选的，因为日志并不是系统正常运转的必要基础，但是有了日志可以更有效理解系统当前的状态，出现问题后通过日志可以高效定位，所以是间接提升了系统的可用性(通过人力间接提高)，系统的高可用，不能只考虑到服务器，运维人员同样是考虑对象，任何可以帮助运维人员减少误操作，或提升恢复效率的努力都是值得的. 确保系统中有 rsyslog 包++123[root@h101 ~]# rpm -qa | grep rsyslogrsyslog-5.8.10-8.el6.x86_64[root@h101 ~]# Tip: Centos6 以后系统都默认使用 rsyslog 来管理日志，当前的最新版为 rsyslog-8.16.0 添加haproxy配置当前配置++123456789101112131415[root@h101 ~]# grep -v "^#" /etc/rsyslog.conf | grep -v "^$"$ModLoad imuxsock # provides support for local system logging (e.g. via logger command)$ModLoad imklog # provides kernel logging support (previously done by rklogd)$ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat$IncludeConfig /etc/rsyslog.d/*.conf*.info;mail.none;authpriv.none;cron.none /var/log/messagesauthpriv.* /var/log/securemail.* -/var/log/maillogcron.* /var/log/cron*.emerg *uucp,news.crit /var/log/spoolerlocal7.* /var/log/boot.log$template SpiceTmpl,"%TIMESTAMP%.%TIMESTAMP:::date-subseconds% %syslogtag% %syslogseverity-text%:%msg:::sp-if-no-1st-sp%%msg:::drop-last-lf%\n":programname, startswith, "spice-vdagent" /var/log/spice-vdagent.log;SpiceTmpl[root@h101 ~]# 当前配置中有一条 $IncludeConfig /etc/rsyslog.d/*.conf , 代表所有在 /etc/rsyslog.d/ 中以 conf 结尾的配置会被合并进来，于是为了便于管理，我们单独为 haproxy 创建一个配置文件 ++1234567[root@h101 ~]# vim /etc/rsyslog.d/haproxy.conf [root@h101 ~]# cat /etc/rsyslog.d/haproxy.conf $ModLoad imudp$UDPServerRun 514local0.* /var/log/haproxy.log[root@h101 ~]# Item Comment $ModLoad imudp 加载UDP输入模块 imudp $UDPServerRun 514 使用UDP的514端口(一般默认是使用这个端口，也就是其它程序不明确指定的情况下也是尝试连接这个端口，如果改为其它端口，写日志的程序要在配置里明确指出，否则没法成功写入) local0.* 自定义一个 local0 类别，这个类别的所有级别报警都记录到 /var/log/haproxy.log /var/log/haproxy.log 文件中 重启rsyslog服务++123456789101112[root@h101 ~]# ll /var/log/ha*ls: cannot access /var/log/ha*: No such file or directory[root@h101 ~]# /etc/init.d/rsyslog restart Shutting down system logger: [ OK ]Starting system logger: [ OK ][root@h101 ~]# ll /var/log/ha*-rw------- 1 root root 0 Mar 3 17:32 /var/log/haproxy.log[root@h101 ~]# netstat -antulp | grep 514tcp 0 0 192.168.100.101:22 192.168.100.1:49514 ESTABLISHED 4491/sshd udp 0 0 0.0.0.0:514 0.0.0.0:* 43095/rsyslogd udp 0 0 :::514 :::* 43095/rsyslogd [root@h101 ~]# 可以看到多出了一个日志文件 /var/log/haproxy.log ，同时也打开了 UDP 的 514 端口 测试写日志我们可以使用 logger 命令来测试配置++12345678[root@h101 ~]# logger -it test -p local0.info "test"[root@h101 ~]# ----------[root@h101 ~]# tail -f /var/log/haproxy.log Mar 4 17:40:21 h101 test[44940]: test......... 在一个窗口中输入 logger -it test -p local0.info “test” ， 跟踪 /var/log/haproxy.log 文件可以看到产生了我定制的信息 安装mycat下载++12345678910[root@h101 mycat]# rsync -av root@192.168.100.102:/usr/local/src/mycat/Mycat-server-1.5-GA-20160217103036-linux.tar.gz . root@192.168.100.102's password: receiving incremental file listMycat-server-1.5-GA-20160217103036-linux.tar.gzsent 30 bytes received 11478842 bytes 3279677.71 bytes/sectotal size is 11477321 speedup is 1.00[root@h101 mycat]# lsMycat-server-1.5-GA-20160217103036-linux.tar.gz[root@h101 mycat]# 解压++123456789101112[root@h101 mycat]# tar -zxvf Mycat-server-1.5-GA-20160217103036-linux.tar.gz mycat/bin/wrapper-linux-ppc-64mycat/bin/wrapper-linux-x86-64mycat/bin/wrapper-linux-x86-32mycat/bin/mycat......mycat/bin/rehash.shmycat/bin/xml_to_yaml.shmycat/logs/mycat/catlet/[root@h101 mycat]# 环境确认++12345[root@h101 mycat]# java -versionjava version "1.7.0_65"OpenJDK Runtime Environment (rhel-2.5.1.2.el6_5-x86_64 u65-b17)OpenJDK 64-Bit Server VM (build 24.65-b04, mixed mode)[root@h101 mycat]# 配置mycatwrapper.conf++12345678[root@h101 conf]# cat wrapper.conf | egrep "(Xm|MaxDirectMemorySize)"#wrapper.java.additional.5=-XX:MaxDirectMemorySize=2Gwrapper.java.additional.5=-XX:MaxDirectMemorySize=256m#wrapper.java.additional.10=-Xmx4Gwrapper.java.additional.10=-Xmx512m#wrapper.java.additional.11=-Xms1Gwrapper.java.additional.11=-Xms128m[root@h101 conf]# server.xml、schema.xml、rule.xml、++123456789101112131415161718192021222324252627282930313233343536373839404142434445464748--[server.xml]-------- &lt;user name="cc"&gt; &lt;property name="password"&gt;cc&lt;/property&gt; &lt;property name="schemas"&gt;cctest&lt;/property&gt; &lt;/user&gt;--[schema.xml]-------- &lt;schema name="cctest" checkSQLschema="false" sqlMaxLimit="100"&gt; &lt;table name="catworld" dataNode="sd1,sd2,sd3" rule="mod-long" /&gt; &lt;table name="abc" dataNode="sd1,sd2,sd3,sd5" rule="mod4-long" /&gt; &lt;/schema&gt; &lt;dataNode name="sd1" dataHost="h101" database="my1" /&gt; &lt;dataNode name="sd2" dataHost="h101" database="my2" /&gt; &lt;dataNode name="sd3" dataHost="h101" database="my3" /&gt; &lt;dataNode name="sd5" dataHost="h101" database="my5" /&gt; ... ... &lt;dataHost name="h101" maxCon="100" minCon="10" balance="0" writeType="0" dbType="mysql" dbDriver="native" switchType="1" slaveThreshold="100"&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;writeHost host="h101M1" url="192.168.100.101:3306" user="root" password="mysql"&gt; &lt;!-- can have multi read hosts --&gt; &lt;/writeHost&gt; &lt;/dataHost&gt;--[rule.xml]-------- &lt;tableRule name="mod-long"&gt; &lt;rule&gt; &lt;columns&gt;id&lt;/columns&gt; &lt;algorithm&gt;mod-long&lt;/algorithm&gt; &lt;/rule&gt; &lt;/tableRule&gt; &lt;tableRule name="mod4-long"&gt; &lt;rule&gt; &lt;columns&gt;id&lt;/columns&gt; &lt;algorithm&gt;mod4-long&lt;/algorithm&gt; &lt;/rule&gt; &lt;/tableRule&gt; ... ... &lt;function name="mod-long" class="org.opencloudb.route.function.PartitionByMod"&gt; &lt;!-- how many data nodes --&gt; &lt;property name="count"&gt;3&lt;/property&gt; &lt;/function&gt; &lt;function name="mod4-long" class="org.opencloudb.route.function.PartitionByMod"&gt; &lt;!-- how many data nodes --&gt; &lt;property name="count"&gt;4&lt;/property&gt; &lt;/function&gt; 打开防火墙确保 8066 开启++1234567[root@h101 conf]# iptables -L -nv | grep 8066[root@h101 conf]# vim /etc/sysconfig/iptables[root@h101 conf]# /etc/init.d/iptables reload iptables: Trying to reload firewall rules: [ OK ][root@h101 conf]# iptables -L -nv | grep 8066 0 0 ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 state NEW tcp dpt:8066 [root@h101 conf]# 以相同的方式打开 9066、8888、9999 Port Comment 8066 默认服务端口 8066 9066 默认管理端口 9066 8888 haproxy对外的mycat服务端口 9999 haproxy对外的mycat管理端口 ++123456[root@h101 ~]# iptables -L -nv | grep -E "(8066|9066|8888|9999)" 0 0 ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 state NEW tcp dpt:8066 0 0 ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 state NEW tcp dpt:9066 0 0 ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 state NEW tcp dpt:8888 0 0 ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 state NEW tcp dpt:9999 [root@h101 ~]# 启动mycat++1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[root@h101 bin]# ./mycat start Starting Mycat-server...[root@h101 bin]#[root@h101 bin]# mysql -u cc -p -P 8066 -p -h 192.168.100.101Enter password: Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 1Server version: 5.5.8-mycat-1.5-GA-20160217103036 MyCat Server (OpenCloundDB)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.mysql&gt; show databases;+----------+| DATABASE |+----------+| cctest |+----------+1 row in set (0.00 sec)mysql&gt; use cctest;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; show tables;+------------------+| Tables in cctest |+------------------+| abc || catworld |+------------------+2 rows in set (0.00 sec)mysql&gt; mysql&gt; select * from abc;+----+------+| id | name |+----+------+| 2 | abc || 6 | abc || 10 | abc || 4 | abc || 8 | abc || 1 | abc || 5 | abc || 9 | xxx |+----+------+8 rows in set (0.53 sec)mysql&gt; Tip: 密切关注 mycat.log 和 wrapper.log ，根据日志信息确认启动成功，如果有错误进行相应调整 配置haproxy添加haproxy用户添加一个 haproxy 用户，并赋权 ++1234567891011[root@h101 haproxy]# grep proxy /etc/passwd[root@h101 haproxy]# useradd haproxy[root@h101 haproxy]# grep proxy /etc/passwdhaproxy:x:505:506::/home/haproxy:/bin/bash[root@h101 haproxy]# chown -R haproxy.haproxy /usr/local/haproxy/[root@h101 haproxy]# ll /usr/local/haproxy/total 16drwxr-xr-x 3 haproxy haproxy 4096 Mar 2 16:23 docdrwxr-xr-x 2 haproxy haproxy 4096 Mar 2 16:23 sbindrwxr-xr-x 3 haproxy haproxy 4096 Mar 2 16:23 share[root@h101 haproxy]# 配置haproxy++1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@h101 ~]# cd /usr/local/haproxy/[root@h101 haproxy]# vim haproxy.cfg [root@h101 haproxy]# grep -v "^#" haproxy.cfg global log 127.0.0.1 local0 maxconn 512 chroot /usr/local/haproxy user haproxy group haproxy daemondefaults log global option dontlognull retries 3 option redispatch maxconn 512 timeout connect 5000 timeout client 50000 timeout server 50000listen admin_status bind *:1234 stats uri /admin stats auth admin:admin mode http option httploglisten all_mycat bind *:8888 mode tcp option tcplog balance roundrobin server mycat_101 192.168.100.101:8066 check port 8066 inter 5s rise 2 fall 3 server mycat_102 192.168.100.102:8066 check port 8066 inter 5s rise 2 fall 3 timeout server 20000listen all_mycat_admin bind *:9999 mode tcp option tcplog balance roundrobin server mycat_101 192.168.100.101:9066 check port 9066 inter 5s rise 2 fall 3 server mycat_102 192.168.100.102:9066 check port 9066 inter 5s rise 2 fall 3 timeout server 20000[root@h101 haproxy]# Haproxy的配置有三个来源： 运行时通过命令行指定 配置文件的 global 区域 代理区域，包含 defaults、listen、frontend、backend 时间单位： Units Comment us microseconds 1/1000000 second ms milliseconds 1/1000 second ，这是默认配置，不指定单位时用这个单位 s seconds. 1s = 1000ms m minutes. 1m = 60s = 60000ms h hours. 1h = 60m = 3600s = 3600000ms d days. 1d = 24h = 1440m = 86400s = 86400000ms CONF Comment log 127.0.0.1 local0 记录到本机的 local0 类别中(之前rsyslog中自定义的类别)，详细可参考 log maxconn 512 限制 最大并行连接数 为512，详细可参考 maxconn chroot /usr/local/haproxy 限定进程的 视域 ，详细可参考 chroot ，官方文档说 It is important to ensure that dir is both empty and unwritable to anyone，然而实际并无此限制，强调这个只是为了安全考虑 user haproxy 以haproxy的身份运行 详细可参考 user group haproxy 以haproxy的组运行 详细可参考 group daemon 后台服务的方式运行， 详细可参考 daemon log global 日志配置沿袭global的设定, 详细可参考 log global option dontlognull 不记录无数据的操作，比如监控的侦测包 ,详细可参考 dontlognull retries 3 失败后最多重试3次, 详细可参考 retries option redispatch 跳转的设定，-1代表最后一次失败尝试就直接跳转，1代表每一次失败尝试都是跳转，0代表失败后不进行跳转, 详细可参考 redispatch timeout connect 5000 一般性的超时时间为5s , 详细可参考 timeout connect timeout client 50000 与客户端之间的超时时间为50s ,详细可参考 timeout client timeout server 50000 与后端服务器之间的超时时间为50s ,详细可参考 timeout server listen admin_status 定义一个包含前后端的完整监听 bind *:1234 监听1234端口, * 代表任意IP, 详细可参考 bind stats uri /admin 定义/admin为统计uri， 详细可参考 stats uri stats auth admin:admin 定义认证用户名和密码为 admin:admin ， 详细可参考 stats auth mode http 设定代理模式为http， 详细可参考 mode option httplog 指定日志模式为http模式 ， 详细可参考 option httplog balance roundrobin 以轮询的方式进行负载均衡， 详细可参考 balance server mycat_101 192.168.100.101:8066 check port 8066 inter 5s rise 2 fall 3 定义一个后端服务器为mycat_101，主机IP为192.168.100.101 , 端口为 8066，检查8066是否开放，检查周期为5s，连续两次检查成功算正常，连续三次检查失败算异常 Tip: Mycat 官方文档中的配置是使用的 contimeout、clitimeout、srvtimeout ，这种写法已经不被支持，如果在配置中这样指定会有如下报错 ++12345[root@h101 haproxy]# /usr/local/haproxy/sbin/haproxy -f /usr/local/haproxy/haproxy.cfg[WARNING] 063/215627 (16321) : parsing [/usr/local/haproxy/haproxy.cfg:16] : the 'contimeout' directive is now deprecated in favor of 'timeout connect', and will not be supported in future versions.[WARNING] 063/215627 (16321) : parsing [/usr/local/haproxy/haproxy.cfg:18] : the 'clitimeout' directive is now deprecated in favor of 'timeout client', and will not be supported in future versions.[WARNING] 063/215627 (16321) : parsing [/usr/local/haproxy/haproxy.cfg:20] : the 'srvtimeout' directive is now deprecated in favor of 'timeout server', and will not be supported in future versions.[root@h101 haproxy]# 正确的写法是++123456#contimeout 5000 timeout connect 5000#clitimeout 50000 timeout client 50000#srvtimeout 50000 timeout server 50000 Tip: mycat官方文档中是使用 listen all_mycat 192.168.100.101:8888 的方式对ip进行绑定，但这种方式已经不被支持，如果使用这种方式，会有如下报错 ++1234567891011121314[root@h101 haproxy]# /usr/local/haproxy/sbin/haproxy -f /usr/local/haproxy/haproxy.cfg[ALERT] 063/223814 (22295) : parsing [/usr/local/haproxy/haproxy.cfg:29] : 'listen' cannot handle unexpected argument '192.168.100.101:8888'.[ALERT] 063/223814 (22295) : parsing [/usr/local/haproxy/haproxy.cfg:29] : please use the 'bind' keyword for listening addresses.[WARNING] 063/223814 (22295) : parsing [/usr/local/haproxy/haproxy.cfg:36] : overwriting 'timeout server' which was already specified[ALERT] 063/223814 (22295) : Error(s) found in configuration file : /usr/local/haproxy/haproxy.cfg[WARNING] 063/223814 (22295) : config : proxy 'all_mycat' has no 'bind' directive. Please declare it as a backend if this was intended.[WARNING] 063/223814 (22295) : config : missing timeouts for proxy 'all_mycat'. | While not properly invalid, you will certainly encounter various problems | with such a configuration. To fix this, please ensure that all following | timeouts are set to a non-zero value: 'client', 'connect', 'server'.[WARNING] 063/223814 (22295) : config : log format ignored for proxy 'all_mycat' since it has no log address.[ALERT] 063/223814 (22295) : Fatal errors found in configuration.[root@h101 haproxy]# 正确的写法是使用 bind ++12345678listen all_mycat bind *:8888 mode tcp option tcplog balance roundrobin server mycat_101 192.168.100.101:8066 check port 8066 inter 5s rise 2 fall 3 server mycat_102 192.168.100.102:8066 check port 8066 inter 5s rise 2 fall 3 timeout server 20000 详细内容可以参考 Haproxy 配置 启动haproxy++12345678910[root@h101 haproxy]# /usr/local/haproxy/sbin/haproxy -f /usr/local/haproxy/haproxy.cfg[root@h101 haproxy]# ps faux | grep -v grep | grep haproxyhaproxy 23083 0.0 0.0 14260 936 ? Ss 22:43 0:00 /usr/local/haproxy/sbin/haproxy -f /usr/local/haproxy/haproxy.cfg[root@h101 haproxy]# netstat -ant | grep -E "(8066|9066|8888|9999|1234)"tcp 0 0 0.0.0.0:1234 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:8888 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:9999 0.0.0.0:* LISTEN tcp 0 0 :::8066 :::* LISTEN tcp 0 0 :::9066 :::* LISTEN [root@h101 haproxy]# 查看 /var/log/haproxy.log 日志，会多出如下记录 ++123Mar 4 22:43:40 localhost haproxy[23081]: Proxy admin_status started.Mar 4 22:43:40 localhost haproxy[23081]: Proxy all_mycat started.Mar 4 22:43:40 localhost haproxy[23081]: Proxy all_mycat_admin started. Tip: 这里只演示了其中一台的配置方法，以上这些在另一台服务器上也要作同样的设置 监控haproxy浏览器中输入 http://192.168.100.101:1234/admin ，进行查看 如果检测到其中一个 mycat异常，会是如下界面 ++1234567[root@h101 ~]# ps faux | grep mycatroot 33274 0.0 0.0 103256 828 pts/2 S+ 23:55 0:00 | \_ grep mycatroot 3980 0.1 0.0 19124 784 ? Sl 13:17 0:50 /usr/local/src/mycat/mycat/bin/./wrapper-linux-x86-64 /usr/local/src/mycat/mycat/conf/wrapper.conf wrapper.syslog.ident=mycat wrapper.pidfile=/usr/local/src/mycat/mycat/logs/mycat.pid wrapper.daemonize=TRUE wrapper.lockfile=/var/lock/subsys/mycat[root@h101 ~]# kill 3980[root@h101 ~]# ps faux | grep mycatroot 33292 0.0 0.0 103256 828 pts/2 S+ 23:55 0:00 | \_ grep mycat[root@h101 ~]# Tip: 要确保在本地防火墙上开放了 1234 端口 配置keepalived简单的haproxy检查脚本keepalived要对本机运行的haproxy健康状态进行检查，当发现haproxy不能正常工作的情况下，将IP交由另一台服务器进行管理 下面是一个最简单的 haproxy 健康检查脚本，能实现对haproxy运行状态的粗略判断(当然这个脚本有很大的精进打磨空间)++123456789101112[root@h101 script]# cat /usr/local/keepalived/script/chk_haproxy.bash #!/bin/bashcount=`ps aux | grep -v grep | grep haproxy.cfg | wc -l`if [ $count -gt 0 ]; then exit 0else exit 1fi[root@h101 script]# chmod +x /usr/local/keepalived/script/chk_haproxy.bash [root@h101 script]# 它进行的判断就是，如果系统中有命令包含 haproxy.cfg 的进程(假定这种情况就代表haproxy正在运行)，就反馈 0 ， 否则反馈 1 ++1234567891011[root@h101 script]# ps faux | grep -v grep | grep haproxy haproxy 23083 0.0 0.0 14260 1408 ? Ss 22:43 0:00 /usr/local/haproxy/sbin/haproxy -f /usr/local/haproxy/haproxy.cfg[root@h101 script]# /usr/local/keepalived/script/chk_haproxy.bash [root@h101 script]# echo $?0[root@h101 script]# kill 23083[root@h101 script]# ps faux | grep -v grep | grep haproxy [root@h101 script]# /usr/local/keepalived/script/chk_haproxy.bash [root@h101 script]# echo $?1[root@h101 script]# 配置keepalived++1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465[root@h101 script]# cat /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123; router_id LVS_101&#125;vrrp_script checkhaproxy &#123; script "/usr/local/keepalived/script/chk_haproxy.bash" weight -20 interval 3&#125;vrrp_instance VI_222 &#123; state BACKUP interface eth2 virtual_router_id 222 priority 108 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; track_script &#123; checkhaproxy &#125; virtual_ipaddress &#123; 192.168.100.222/24 &#125;&#125;[root@h101 script]# ----------[root@h102 ~]# cat /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123; router_id LVS_102&#125;vrrp_script checkhaproxy &#123; script "/usr/local/keepalived/script/chk_haproxy.bash" weight -20 interval 3&#125;vrrp_instance VI_222 &#123; state BACKUP interface eth2 virtual_router_id 222 priority 115 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; track_script &#123; checkhaproxy &#125; virtual_ipaddress &#123; 192.168.100.222/24 &#125;&#125;[root@h102 ~]# 其中的核心部分在这里++12345678910vrrp_script checkhaproxy &#123; script "/usr/local/keepalived/script/chk_haproxy.bash" weight -20 interval 3&#125;...... track_script &#123; checkhaproxy &#125; track_script 中调用 checkhaproxy checkhaproxy 的意思是： 每3秒种执行一次 /usr/local/keepalived/script/chk_haproxy.bash 脚本 如果反馈结果是 0，就保持原优先级 priority ，如果是 1 ，就将优先级降低 20，也就是检查到 haproxy 状态异常后，就降级，以便让另一台服务器的keepalived进程可以抢到IP 为了避免网络的不稳定还可以加入 fall N (代表连续N次检查失败才算异常) 和 rise N (代表连续N次检查成功就算正常) 优先级改变的算法是这样的： 如果脚本执行结果为0，并且weight配置的值大于0，则优先级相应的增加 如果脚本执行结果非0，并且weight配置的值小于0，则优先级相应的减少 其他情况，维持原本配置的优先级，即配置文件中priority对应的值 Tip: keepalived 相互之间的通讯要使用到组播，如果没打开，会出现几个实例同时抢占着IP的情况，打开方式是在iptables中加入 -A INPUT -d 224.0.0.18 -j ACCEPT，然后重载iptables配置 启动keepalived先确保两边的haproxy都是正常运行的++12[root@h101 script]# /usr/local/keepalived/sbin/keepalived -f /etc/keepalived/keepalived.conf[root@h101 script]# 两边的keepalived启动后，以初始设定优先级高的keepalived为Master 当优先级高的keeaplived检测到haproxy异常后，会自动降级20，然后重新选举Master，这时另一台服务器的优先级就相对较高，更有优势，于是抢到IP，成为新的Master 在原master上执行以下命令，可以看到IP的漂移过程++12345678910111213141516171819202122[root@h102 mycat-web]# ip a 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:b6:a8:f8 brd ff:ff:ff:ff:ff:ff inet 192.168.1.143/24 brd 192.168.1.255 scope global eth3 inet6 fe80::20c:29ff:feb6:a8f8/64 scope link valid_lft forever preferred_lft forever3: eth2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:b6:a8:02 brd ff:ff:ff:ff:ff:ff inet 192.168.100.102/24 brd 192.168.100.255 scope global eth2 inet 192.168.100.222/24 scope global secondary eth2 inet6 fe80::20c:29ff:feb6:a802/64 scope link valid_lft forever preferred_lft forever[root@h102 mycat-web]# ps faux | grep haproxy root 12707 0.0 0.0 103256 828 pts/1 S+ 23:48 0:00 | \_ grep haproxyhaproxy 12118 0.0 0.0 14260 936 ? Ss 23:42 0:00 /usr/local/haproxy/sbin/haproxy -f /usr/local/haproxy/haproxy.cfg[root@h102 mycat-web]# kill 12118; watch -n .5 ip a [root@h102 mycat-web]# 这个过程中 192.168.100.222 会消失，在另一台服务器上，就能看到这个IP被挂载了 反过来也一样，会看到IP被挂载的过程++123456789101112131415161718[root@h102 mycat-web]# ip a 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:b6:a8:f8 brd ff:ff:ff:ff:ff:ff inet 192.168.1.143/24 brd 192.168.1.255 scope global eth3 inet6 fe80::20c:29ff:feb6:a8f8/64 scope link valid_lft forever preferred_lft forever3: eth2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:b6:a8:02 brd ff:ff:ff:ff:ff:ff inet 192.168.100.102/24 brd 192.168.100.255 scope global eth2 inet6 fe80::20c:29ff:feb6:a802/64 scope link valid_lft forever preferred_lft forever[root@h102 mycat-web]# /usr/local/haproxy/sbin/haproxy -f /usr/local/haproxy/haproxy.cfg; watch -n .5 ip a [root@h102 mycat-web]# 访问测试++1234567891011121314151617181920212223242526272829303132333435363738[root@h101 ~]# mysql -u cc -p -P 8888 -h 192.168.100.222 Enter password: Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 174Server version: 5.5.8-mycat-1.5-GA-20160217103036 MyCat Server (OpenCloundDB)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.mysql&gt; show databases;+----------+| DATABASE |+----------+| cctest |+----------+1 row in set (0.03 sec)mysql&gt; use cctest;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; show tables;+------------------+| Tables in cctest |+------------------+| abc || catworld |+------------------+2 rows in set (0.00 sec)mysql&gt; 切换过程中的影响切换过程并非完全没有任何影响，一般会产生一次中断，但当再次发起请求时(重试时)就能恢复正常 下面的过程就是在切换中进行操作的++12345678910111213141516171819202122232425mysql&gt; show tables;ERROR 2013 (HY000): Lost connection to MySQL server during querymysql&gt; show tables;ERROR 2006 (HY000): MySQL server has gone awayNo connection. Trying to reconnect...Connection id: 175Current database: cctest+------------------+| Tables in cctest |+------------------+| abc || catworld |+------------------+2 rows in set (0.07 sec)mysql&gt; show databases;+----------+| DATABASE |+----------+| cctest |+----------+1 row in set (0.00 sec)mysql&gt; 命令汇总 wget http://www.keepalived.org/software/keepalived-1.2.19.tar.gz tar -zxvf keepalived-1.2.19.tar.gz cd keepalived-1.2.19 ./configure –prefix=/usr/local/keepalived make make install tree /usr/local/keepalived/ /usr/local/keepalived/sbin/keepalived -v md5sum haproxy-1.6.3.tar.gz tar -zxvf haproxy-1.6.3.tar.gz cd haproxy-1.6.3 make TARGET=linux2628 ARCH=x86_64 PREFIX=/usr/local/haproxy make install PREFIX=/usr/local/haproxy tree /usr/local/haproxy/ /usr/local/haproxy/sbin/haproxy -v rpm -qa | grep rsyslog grep -v “^#” /etc/rsyslog.conf | grep -v “^$” vim /etc/rsyslog.d/haproxy.conf cat /etc/rsyslog.d/haproxy.conf /etc/init.d/rsyslog restart ll /var/log/ha* netstat -antulp | grep 514 logger -it test -p local0.info “test” tail -f /var/log/haproxy.log rsync -av root@192.168.100.102:/usr/local/src/mycat/Mycat-server-1.5-GA-20160217103036-linux.tar.gz . tar -zxvf Mycat-server-1.5-GA-20160217103036-linux.tar.gz java -version cat wrapper.conf | egrep “(Xm|MaxDirectMemorySize)” vim /etc/sysconfig/iptables /etc/init.d/iptables reload iptables -L -nv | grep -E “(8066|9066|8888|9999)” ./mycat start mysql -u cc -p -P 8066 -p -h 192.168.100.101 useradd haproxy grep proxy /etc/passwd chown -R haproxy.haproxy /usr/local/haproxy/ vim haproxy.cfg grep -v “^#” haproxy.cfg /usr/local/haproxy/sbin/haproxy -f /usr/local/haproxy/haproxy.cfg ps faux | grep -v grep | grep haproxy netstat -ant | grep -E “(8066|9066|8888|9999|1234)” cat /usr/local/keepalived/script/chk_haproxy.bash chmod +x /usr/local/keepalived/script/chk_haproxy.bash ps faux | grep -v grep | grep haproxy /usr/local/keepalived/script/chk_haproxy.bash cat /etc/keepalived/keepalived.conf /usr/local/keepalived/sbin/keepalived -f /etc/keepalived/keepalived.conf kill 12118; watch -n .5 ip a /usr/local/haproxy/sbin/haproxy -f /usr/local/haproxy/haproxy.cfg; watch -n .5 ip a mysql -u cc -p -P 8888 -h 192.168.100.222]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[DB Cache 联动更新方案]]></title>
      <url>%2F2017%2F05%2F12%2FDB-Cache-%E8%81%94%E5%8A%A8%E6%9B%B4%E6%96%B0%E6%96%B9%E6%A1%88%2F</url>
      <content type="text"><![CDATA[优秀的人，不是不合群，而是他们合群的人里面没有你 作为一个技术人,十分厌烦重复的工作,巴不得把所有的东西做成组件化,通用化。至此，本文推出一篇基于DB &lt;==&gt; Cache 联动的博文，并给出解决方案。 业务需求说明几乎所有的业务都会用到缓存机制，存放临时的常规数据。同时，使用了缓存，也就会有调用接口或者是查询数据库的操作；我们都会进行一个必要的操作： 调用前检查缓存是否存在数据 执行正常的业务,获取必要的数据 持久化的缓存 问题说明 最近做的一个项目中很多地方用到了Redis，其中纠结了一下redis的数据持久化问题，毕竟是缓存，数据随时都有可能丢失，虽然概率不大，况且redis还会将数据持久到安装路径的一个文件中，但还是要保证缓存数据与持久化数据的一致性，这个问题总结了一下（看到了一个不错的博文），其实就是读和写，还有就是要注意谁先谁后的问题。Redis 是一个高性能的key-value数据库。redis的出现，很大程度补偿了memcached这类keyvalue存储的不足，在部分场合可以对关系数据库起到很好的补充作用。它提供了Python，Ruby，Erlang，PHP客户端，使用很方便。 1.按照我们一般的使用Redis的场景应该是这样的： 也就是说：我们会先去redis中判断数据是否存在，如果存在，则直接返回缓存好的数据。而如果不存在的话，就会去数据库中，读取数据，并把数据缓存到Redis中。适用场合：如果数据量比较大，但不是经常更新的情况(比如用户排行) 2.而第二种Redis的使用，跟第一种的情况完成不同，具体的情况请看： 这里我们会先去redis中判断数据是否存在，如果存在，则直接更新对应的数据(这一步会把对应更新过的key记录下来，比如也保存到redis中比如：key为：save_update_keys【用lpush列表记录】)，并把更新后的数据返回给页面。而如果不存在的话，就会去先更新数据库中内容，然后把数据保存一份到Redis中。后面的工作：后台会有相关机制把Redis中的save_update_keys存储的key，分别读取出来，找到对应的数据，更新到DB中。 优点：这个流程的主要目的是把Redis当作数据库使用，更新获取数据比DB快。非常适合大数据量的频繁变动(比如微博)。缺点：对Redis的依赖很大，要做好宕机时的数据保存。(不过可以使用redis的快照AOF，快速恢复的话，应该不会有多大影响，因为就算Redis不工作了，也不会影响后续数据的处理。)难点：在前期规划key的格式，存储类型很重要，因为这会影响能否把数据同步到DB。 问题总结综上所述,繁琐的流程操作，先查询缓存，断言是否存在，再进行必要的业务操作，存放数据到缓存中心。基本所有的接口都需要这样处理，这会让一个繁琐的工作。我们需要进行组件化处理。包括,后续的雪崩处理，缓存击穿处理等操作，都可以做成一体化处理。简单方便，便民操作，从恶心的工作内容中，挣脱出来。 解决方案初步的方案，是使用aop拦截的方案(相当没技术含量的玩意，然而却能解决问题)。 定义自定义注解1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package com.candao.dms.framework.annotation;import java.lang.annotation.Documented;import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;import com.candao.dms.framework.bean.CacheOPEnum;/** * 自定义注解,进行DB和Cache的联动处理 * * @Retention 用于描述注解的生命周期 * @Target 用于描述注解的使用范围 * * @author jeromeLiu */@Retention(RetentionPolicy.RUNTIME)@Target(&#123; ElementType.METHOD, ElementType.TYPE&#125;)@Documentedpublic @interface CacheLinkDB &#123; /** * 缓存key值前缀 * * @return */ String keyPre(); /** * 缓存key的数据结构,默认是字符串类型 * * 结合当前的业务需求,仅支持字符串类型处理 * * @return */ String keyClassType() default "java.lang.String"; /** * 操作指令(SELECT,DEL,UPDATE),仅作用在方法头部有效 * * @return */ CacheOPEnum OP() default CacheOPEnum.SELECT; /** * 缓存key的失效时间(s),默认失效时间：600s * * @return */ int expire() default 600;&#125; 定义拦截类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133package com.candao.dms.framework.aspect;import java.lang.reflect.Method;import org.apache.commons.lang.StringUtils;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.AfterReturning;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Pointcut;import org.aspectj.lang.reflect.MethodSignature;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.stereotype.Component;import com.candao.dms.framework.annotation.CacheLinkDB;import com.candao.dms.framework.bean.CacheKey;import com.candao.dms.framework.util.ObjectUtil;/** * @author jeromeLiu * * Cache&lt;==&gt;DB 联动切面 */@Component@Aspectpublic class CacheLinkDBAspect &#123; @Autowired public RedisTemplate&lt;String, Object&gt; redisTemplate; @Pointcut("@annotation(com.candao.dms.framework.annotation.CacheLinkDB)") public void checkCache() &#123;&#125; private StringBuffer CONST_REDIS_KEY = new StringBuffer(); @Around("checkCache()") public Object checkCache(ProceedingJoinPoint pjp) throws Throwable &#123; // 获取自定义注解头部信息 MethodSignature signature = (MethodSignature) pjp.getSignature(); Method method = signature.getMethod(); CacheLinkDB action = method.getAnnotation(CacheLinkDB.class); Class&lt;?&gt; classType = Class.forName(action.keyClassType()); CONST_REDIS_KEY = getCacheKey(pjp, action); // 先行判定操作指令 switch (action.OP()) &#123; case SELECT: // 断言对象基础类型 if (classType.getTypeName() instanceof String) &#123; Object value = redisTemplate.opsForValue().get(CONST_REDIS_KEY.toString()); if (value != null &amp;&amp; StringUtils.isNotEmpty(value.toString()) &amp;&amp; StringUtils.isNotBlank(value.toString())) &#123; return value; &#125; &#125; break; case DEL: redisTemplate.delete(CONST_REDIS_KEY.toString()); break; case UPDATE: redisTemplate.delete(CONST_REDIS_KEY.toString()); break; case SAVE: //保险操作,先进行清理操作 redisTemplate.delete(CONST_REDIS_KEY.toString()); System.out.println("set操作,无须做任何操作."); break; &#125; // 执行正常的业务操作 return pjp.proceed(); &#125; /** * @param jp * 切面对象 * @param rtv * 执行结果返回值 */ @AfterReturning(pointcut = "checkCache()", returning = "rtv") public void saveDataWithCache(JoinPoint jp, Object rtv) &#123; Object value = redisTemplate.opsForValue().get(CONST_REDIS_KEY.toString()); // 断言redis是否存在key if (value == null &amp;&amp; rtv != null) &#123; redisTemplate.opsForValue().set(CONST_REDIS_KEY.toString(), ObjectUtil.toString(rtv)); &#125; &#125; /** * 构造cacheKey结构 * * @param pjp * @param action * @return * @throws Throwable */ private StringBuffer getCacheKey(ProceedingJoinPoint pjp, CacheLinkDB action) throws Throwable &#123; // 递归方法形参 for (Object object : pjp.getArgs()) &#123; Boolean isSuperClass = CacheKey.class.isAssignableFrom(object.getClass().getSuperclass()); // 1.先断言是否存在CacheKey对象 的父类 if (!isSuperClass) &#123; // 该处说明下,必须是自身的对象携带id列段,通过继承的方式是无效的(暂不考虑继承的方案). Method getIdMethod = object.getClass().getMethod("getId"); // 1.1 断言是否存在id列段 if (getIdMethod == null) continue; // 1.2 反射执行函数获取返回值 String id = (String) getIdMethod.invoke(object); CacheLinkDB cacheLinkDB = object.getClass().getAnnotation(CacheLinkDB.class); // 1.3 断言对象是否存在指定的注解对象CacheLinkDB if (cacheLinkDB != null) &#123; CONST_REDIS_KEY.append(cacheLinkDB.keyPre()).append("_").append(id); &#125; else &#123; // 1.4 使用外围方法的keyPre CONST_REDIS_KEY.append(action.keyPre()).append("_").append(id); &#125; break; &#125; else &#123; // 2.反射执行函数获取返回值 Method getCacheKeyMethod = object.getClass().getMethod("getCacheKey"); String cacheKey = (String) getCacheKeyMethod.invoke(object); CONST_REDIS_KEY.append(cacheKey); break; &#125; &#125; return CONST_REDIS_KEY; &#125;&#125; 定义操作指令枚举1234567891011121314151617package com.candao.dms.framework.bean;/** * 缓存操作指令 * * @author jeromeLiu * @version 1.0.0 2017年5月12日 上午10:37:00 */public enum CacheOPEnum &#123; SELECT, UPDATE, DEL, SAVE&#125; 定义自定义的CacheKey1234567891011121314151617package com.candao.dms.framework.bean;/** * * * @author jeromeLiu * @version 1.0.0 2017年5月11日 下午3:10:24 */public abstract class CacheKey &#123; /** * key 规则 ： xxx:id(code) */ public abstract String getCacheKey();&#125; 定义测试Bean1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.candao.dms.demo2.dbcache;import com.alibaba.fastjson.annotation.JSONField;import com.candao.dms.framework.annotation.CacheLinkDB;import com.candao.dms.framework.bean.CacheKey;/** * @author jeromeLiu * @version 1.0.0 2017年5月11日 下午3:13:32 */@CacheLinkDB(keyPre = "jeromeLiu", keyClassType = "java.lang.String")public class JeromeLiu extends CacheKey &#123; private String id; private String name; private Integer age; public String getId() &#123; return id; &#125; public void setId(String id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; @Override @JSONField(serialize=false) public String getCacheKey() &#123; return "jeromeLiu:id:" + getId(); &#125;&#125; 业务操作类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package com.candao.dms.demo2.dbcache;import org.springframework.stereotype.Service;import com.alibaba.fastjson.JSONObject;import com.candao.dms.framework.annotation.CacheLinkDB;import com.candao.dms.framework.bean.CacheOPEnum;/** * @author jeromeLiu */@Servicepublic class JeromeLiuTest &#123; @CacheLinkDB(keyPre = "jeromeLiu", OP = CacheOPEnum.SELECT, keyClassType = "java.lang.String") public String testAnnotation() &#123; return "查询数据库"; &#125; /** * 测试查询行为 * * @param jeromeLiu * @return */ @CacheLinkDB(keyPre = "jeromeLiu", OP = CacheOPEnum.SELECT, keyClassType = "java.lang.String") public String testAnnotation2(JeromeLiu jeromeLiu) &#123; System.out.println("走DB：我是查询行为"); return JSONObject.toJSONString(jeromeLiu); &#125; /** * 测试更新行为 * * @param jeromeLiu * @return */ @CacheLinkDB(keyPre = "jeromeLiu", OP = CacheOPEnum.UPDATE, keyClassType = "java.lang.String") public String testAnnotation3(JeromeLiu jeromeLiu) &#123; System.out.println("走DB：我是更新行为"); return JSONObject.toJSONString(jeromeLiu); &#125; /** * 测试删除行为 * * @param jeromeLiu * @return */ @CacheLinkDB(keyPre = "jeromeLiu", OP = CacheOPEnum.DEL, keyClassType = "java.lang.String") public String testAnnotation4(JeromeLiu jeromeLiu) &#123; System.out.println("走DB：我是删除行为"); return JSONObject.toJSONString(jeromeLiu); &#125;&#125; 定义单元测试1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package com.candao.dms.demo2;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;import com.candao.dms.DemoServiceApplication2;import com.candao.dms.demo2.dbcache.JeromeLiu;import com.candao.dms.demo2.dbcache.JeromeLiuTest;/** * * * @author jeromeLiu * @version 1.0.0 2017年5月11日 下午2:34:28 */@RunWith(SpringJUnit4ClassRunner.class)@SpringBootTest(classes = DemoServiceApplication2.class)public class DBLinkCacheTest &#123; @Autowired private JeromeLiuTest jeromeLiuTest; @Test public void test2()&#123; JeromeLiu jeromeLiu = new JeromeLiu(); jeromeLiu.setId("1140120103"); jeromeLiu.setName("刘练源"); jeromeLiu.setAge(25); System.out.println(jeromeLiuTest.testAnnotation2(jeromeLiu)); &#125; @Test public void test3()&#123; JeromeLiu jeromeLiu = new JeromeLiu(); jeromeLiu.setId("1140120103"); jeromeLiu.setName("刘练源"); jeromeLiu.setAge(26); System.out.println(jeromeLiuTest.testAnnotation3(jeromeLiu)); &#125; @Test public void test4()&#123; JeromeLiu jeromeLiu = new JeromeLiu(); jeromeLiu.setId("1140120103"); jeromeLiu.setName("jeromeLiu"); jeromeLiu.setAge(26); System.out.println(jeromeLiuTest.testAnnotation4(jeromeLiu)); &#125;&#125; 附带上工程地址]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[百度分布式ID 构造springcloud工程]]></title>
      <url>%2F2017%2F05%2F08%2F%E7%99%BE%E5%BA%A6%E5%88%86%E5%B8%83%E5%BC%8FID-%E6%9E%84%E9%80%A0springcloud%E5%B7%A5%E7%A8%8B%2F</url>
      <content type="text"><![CDATA[优秀的人，不是不合群，而是他们合群的人里面没有你 最近正在研究高可用分布式全局ID的生成策略，网上很多种方案和实现。主要集中在几个方面： 数据库自增长序列或字段 UUID Redis生产ID Zookeeper的叶子方案生产ID Twitter的snowflake算法 MongoDB的ObjectId 第三方方案(百度,美团) 本文并不打算详解各个方案的异同和使用方式，各位看官可以根据自己的喜欢进行选择。本文详细说明把百度的uid-generator切换成SpringCloud方案 工程解剖百度官方uid-generator项目结构： 创建工程根据SpringCloud零配置规则,新建springcloud工程。引入springcould相关maven配置：1234567891011121314151617181920212223242526272829303132333435&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper&lt;/artifactId&gt; &lt;version&gt;4.1.6&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.19&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-core&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;&lt;/dependency&gt; 配置spring-mybatis123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137package com.candao.dms.uid.datasource;import java.sql.SQLException;import java.util.Arrays;import org.apache.ibatis.session.ExecutorType;import org.apache.ibatis.session.SqlSessionFactory;import org.mybatis.spring.SqlSessionFactoryBean;import org.mybatis.spring.SqlSessionTemplate;import org.mybatis.spring.annotation.MapperScan;import org.mybatis.spring.mapper.MapperScannerConfigurer;import org.springframework.boot.bind.RelaxedPropertyResolver;import org.springframework.context.ApplicationContextException;import org.springframework.context.EnvironmentAware;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.core.env.Environment;import org.springframework.core.io.support.PathMatchingResourcePatternResolver;import org.springframework.jdbc.datasource.DataSourceTransactionManager;import org.springframework.stereotype.Repository;import org.springframework.transaction.PlatformTransactionManager;import org.springframework.transaction.annotation.EnableTransactionManagement;import org.springframework.util.StringUtils;import com.alibaba.druid.pool.DruidDataSource;/** * Created by jeromeLiu */@Configuration@EnableTransactionManagement@MapperScan(value = "com.candao.dms.uid")public class DatabaseConfiguration implements EnvironmentAware &#123; private Environment environment; private RelaxedPropertyResolver propertyResolver; @Override public void setEnvironment(Environment environment) &#123; this.environment = environment; this.propertyResolver = new RelaxedPropertyResolver(environment, "spring.datasource."); &#125; // 注册dataSource @Bean(destroyMethod = "close") public DruidDataSource dataSource() throws SQLException &#123; if (StringUtils.isEmpty(propertyResolver.getProperty("url"))) &#123; System.out.println("Your database connection pool configuration is incorrect!" + " Please check your Spring profile, current profiles are:" + Arrays.toString(environment.getActiveProfiles())); throw new ApplicationContextException("Database connection pool is not configured correctly"); &#125; DruidDataSource druidDataSource = new DruidDataSource(); druidDataSource.setMaxActive(Integer.parseInt(propertyResolver.getProperty("maxActive"))); druidDataSource.setDriverClassName(propertyResolver.getProperty("driver-class-name")); druidDataSource.setUrl(propertyResolver.getProperty("url")); druidDataSource.setUsername(propertyResolver.getProperty("username")); druidDataSource.setPassword(propertyResolver.getProperty("password")); druidDataSource.setInitialSize(Integer.parseInt(propertyResolver.getProperty("initialSize"))); druidDataSource.setDefaultAutoCommit(Boolean.parseBoolean(propertyResolver.getProperty("defaultAutoCommit"))); druidDataSource.setMinIdle(Integer.parseInt(propertyResolver.getProperty("minIdle"))); druidDataSource.setMaxWait(Integer.parseInt(propertyResolver.getProperty("maxWait"))); druidDataSource.setTestWhileIdle(Boolean.parseBoolean(propertyResolver.getProperty("testWhileIdle"))); druidDataSource.setTestOnBorrow(Boolean.parseBoolean(propertyResolver.getProperty("testOnBorrow"))); druidDataSource.setTestOnReturn(Boolean.parseBoolean(propertyResolver.getProperty("testOnReturn"))); druidDataSource.setValidationQuery(propertyResolver.getProperty("validationQuery")); druidDataSource.setTimeBetweenEvictionRunsMillis(Long.parseLong(propertyResolver.getProperty("timeBetweenEvictionRunsMillis"))); druidDataSource.setMinEvictableIdleTimeMillis(Long.parseLong(propertyResolver.getProperty("minEvictableIdleTimeMillis"))); druidDataSource.setLogAbandoned(Boolean.parseBoolean(propertyResolver.getProperty("logAbandoned"))); druidDataSource.setRemoveAbandoned(Boolean.parseBoolean(propertyResolver.getProperty("removeAbandoned"))); druidDataSource .setRemoveAbandonedTimeout(Integer.parseInt(propertyResolver.getProperty("removeAbandonedTimeout"))); druidDataSource.setFilters(propertyResolver.getProperty("filters")); // druidDataSource.setPoolPreparedStatements(Boolean.parseBoolean(propertyResolver.getProperty("poolPreparedStatements"))); // druidDataSource.setMaxPoolPreparedStatementPerConnectionSize(Integer.parseInt(propertyResolver.getProperty("maxPoolPreparedStatementPerConnectionSize"))); return druidDataSource; &#125; /** * 创建SqlSessionFactory，同时指定数据源 * * @return * @throws Exception */ @Bean public SqlSessionFactory sqlSessionFactory() throws Exception &#123; SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean(); sqlSessionFactoryBean.setDataSource(dataSource()); /*Properties props = new Properties(); props.setProperty("dialect", "mysql"); props.setProperty("reasonable", "true"); props.setProperty("supportMethodsArguments", "true"); props.setProperty("returnPageInfo", "check"); props.setProperty("params", "count=countSql"); // mybatis分页 PageHelper pageHelper = new PageHelper(); pageHelper.setProperties(props); // 添加插件 sqlSessionFactoryBean.setPlugins(new Interceptor[] &#123; pageHelper &#125;);*/ PathMatchingResourcePatternResolver resolver = new PathMatchingResourcePatternResolver(); sqlSessionFactoryBean.setMapperLocations(resolver.getResources("classpath*:/mapper/WORKER*.xml")); return sqlSessionFactoryBean.getObject(); &#125; @Bean public MapperScannerConfigurer setMapScanner()&#123; MapperScannerConfigurer mapperScannerConfigurer = new MapperScannerConfigurer(); mapperScannerConfigurer.setAnnotationClass(Repository.class); mapperScannerConfigurer.setBasePackage("com.candao.dms.uid.worker.dao"); mapperScannerConfigurer.setSqlSessionFactoryBeanName("sqlSessionFactory"); return mapperScannerConfigurer; &#125; /** * 配置事务信息 * * @return * @throws SQLException */ @Bean public PlatformTransactionManager transactionManager() throws SQLException &#123; return new DataSourceTransactionManager(dataSource()); &#125; /** * 配置批插 * * @return * @throws Exception */ @Bean public SqlSessionTemplate batchSqlSession() throws Exception&#123; return new SqlSessionTemplate(sqlSessionFactory(), ExecutorType.BATCH); &#125;&#125; 上述的代码，替换了原生方案的mybatis-spring.xml配置文件信息 配置 application.properties1234567891011121314151617181920212223242526272829303132333435363738394041424344#datasource db infospring.datasource.type=com.alibaba.druid.pool.DruidDataSourcespring.datasource.driver-class-name=com.mysql.jdbc.Driverspring.datasource.url=jdbc:mysql://10.200.102.32:3306/uid-generatorspring.datasource.username=rootspring.datasource.password=51wm_testspring.datasource.maxActive=2#datasource basespring.datasource.defaultAutoCommit=truespring.datasource.initialSize=2spring.datasource.minIdle=0spring.datasource.maxWait=5000spring.datasource.testWhileIdle=truespring.datasource.testOnBorrow=truespring.datasource.testOnReturn=falsespring.datasource.validationQuery=SELECT 1 FROM DUALspring.datasource.timeBetweenEvictionRunsMillis=30000spring.datasource.minEvictableIdleTimeMillis=60000spring.datasource.logAbandoned=truespring.datasource.removeAbandoned=truespring.datasource.removeAbandonedTimeout=120spring.datasource.filters=stat#redis base# Redis数据库索引（默认为0）spring.redis.database=0# Redis服务器地址spring.redis.host=127.0.0.1# Redis服务器连接端口spring.redis.port=6379# Redis服务器连接密码（默认为空）#spring.redis.password=jerome# 连接池最大连接数（使用负值表示没有限制）spring.redis.pool.max-active=300# 连接池最大阻塞等待时间（使用负值表示没有限制）spring.redis.pool.max-wait=5000# 连接池中的最大空闲连接spring.redis.pool.max-idle=100# 连接池中的最小空闲连接spring.redis.pool.min-idle=20# 连接超时时间（毫秒）spring.redis.timeout=0 替换bean的xml注入方案uid-generator的项目设计,是通过xml的方式,硬编码的注入实例。原生的描述信息(局部)1234567891011121314151617181920212223242526272829&lt;!-- UID generator --&gt; &lt;bean id="disposableWorkerIdAssigner" class="com.baidu.fsg.uid.worker.DisposableWorkerIdAssigner" /&gt; &lt;bean id="cachedUidGenerator" class="com.baidu.fsg.uid.impl.CachedUidGenerator"&gt; &lt;property name="workerIdAssigner" ref="disposableWorkerIdAssigner" /&gt; &lt;!-- 以下为可选配置, 如未指定将采用默认值 --&gt; &lt;!-- RingBuffer size扩容参数, 可提高UID生成的吞吐量. --&gt; &lt;!-- 默认:3， 原bufferSize=8192, 扩容后bufferSize= 8192 &lt;&lt; 3 = 65536 --&gt; &lt;!--&lt;property name="boostPower" value="3"&gt;&lt;/property&gt;--&gt; &lt;!-- 指定何时向RingBuffer中填充UID, 取值为百分比(0, 100), 默认为50 --&gt; &lt;!-- 举例: bufferSize=1024, paddingFactor=50 -&gt; threshold=1024 * 50 / 100 = 512. --&gt; &lt;!-- 当环上可用UID数量 &lt; 512时, 将自动对RingBuffer进行填充补全 --&gt; &lt;!--&lt;property name="paddingFactor" value="50"&gt;&lt;/property&gt;--&gt; &lt;!-- 另外一种RingBuffer填充时机, 在Schedule线程中, 周期性检查填充 --&gt; &lt;!-- 默认:不配置此项, 即不实用Schedule线程. 如需使用, 请指定Schedule线程时间间隔, 单位:秒 --&gt; &lt;!--&lt;property name="scheduleInterval" value="60"&gt;&lt;/property&gt;--&gt; &lt;!-- 拒绝策略: 当环已满, 无法继续填充时 --&gt; &lt;!-- 默认无需指定, 将丢弃Put操作, 仅日志记录. 如有特殊需求, 请实现RejectedPutBufferHandler接口(支持Lambda表达式) --&gt; &lt;!--&lt;property name="rejectedPutBufferHandler" ref="XxxxYourPutRejectPolicy"&gt;&lt;/property&gt;--&gt; &lt;!-- 拒绝策略: 当环已空, 无法继续获取时 --&gt; &lt;!-- 默认无需指定, 将记录日志, 并抛出UidGenerateException异常. 如有特殊需求, 请实现RejectedTakeBufferHandler接口(支持Lambda表达式) --&gt; &lt;!--&lt;property name="rejectedPutBufferHandler" ref="XxxxYourPutRejectPolicy"&gt;&lt;/property&gt;--&gt; &lt;/bean&gt; 我们可以替换成注解的方案,根据自己想要的实例进行注入就行。详见下面：123456789101112131415@Repositorypublic class CachedUidGenerator extends DefaultUidGenerator implements DisposableBean &#123; private static final Logger LOGGER = LoggerFactory.getLogger(CachedUidGenerator.class);&#125;@Repositorypublic class DefaultUidGenerator implements UidGenerator, InitializingBean &#123; private static final Logger LOGGER = LoggerFactory.getLogger(DefaultUidGenerator.class);&#125;@Repositorypublic class DisposableWorkerIdAssigner implements WorkerIdAssigner &#123; private static final Logger LOGGER = LoggerFactory.getLogger(DisposableWorkerIdAssigner.class);&#125; 测试样例描述：12345678910111213141516171819@RunWith(SpringJUnit4ClassRunner.class)@SpringBootTest(classes = DmsUidServiceApplication.class)public class DmsUidServiceApplicationTests &#123; @Resource(name="cachedUidGenerator") private UidGenerator uidGenerator; /** * 生产单个uid */ @Test public void testSerialGenerate2() &#123; // Generate UID long uid = uidGenerator.getUID(); System.out.println(uid); System.out.println(uidGenerator.parseUID(uid)); &#125;&#125; 至此springcloud的切换到此结束。 百度uid-generator方案改造成springcould]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Kubernetes 搭建ingress]]></title>
      <url>%2F2017%2F04%2F24%2FKubernetes-%E6%90%AD%E5%BB%BAingress%2F</url>
      <content type="text"><![CDATA[优秀的人，不是不合群，而是他们合群的人里面没有你 Ingress 介绍Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress；前两种估计都应该很熟悉。 Ingress 是个什么可能从大致印象上 Ingress 就是能利用 Nginx、Haproxy 啥的负载均衡器暴露集群内服务的工具；那么问题来了，集群内服务想要暴露出去面临着几个问题： Pod 漂移问题众所周知 Kubernetes 具有强大的副本控制能力，能保证在任意副本(Pod)挂掉时自动从其他机器启动一个新的，还可以动态扩容等，总之一句话，这个 Pod 可能在任何时刻出现在任何节点上，也可能在任何时刻死在任何节点上；那么自然随着 Pod 的创建和销毁，Pod IP 肯定会动态变化；那么如何把这个动态的 Pod IP 暴露出去？这里借助于 Kubernetes 的 Service 机制，Service 可以以标签的形式选定一组带有指定标签的 Pod，并监控和自动负载他们的 Pod IP，那么我们向外暴露只暴露 Service IP 就行了；这就是 NodePort 模式：即在每个节点上开起一个端口，然后转发到内部 Service IP 上，如下图所示 端口管理问题采用 NodePort 方式暴露服务面临一个坑爹的问题是，服务一旦多起来，NodePort 在每个节点上开启的端口会及其庞大，而且难以维护；这时候引出的思考问题是 “能不能使用 Nginx 啥的只监听一个端口，比如 80，然后按照域名向后转发？” 这思路很好，简单的实现就是使用 DaemonSet 在每个 node 上监听 80，然后写好规则，因为 Nginx 外面绑定了宿主机 80 端口(就像 NodePort)，本身又在集群内，那么向后直接转发到相应 Service IP 就行了，如下图所示 域名分配及动态更新问题从上面的思路，采用 Nginx 似乎已经解决了问题，但是其实这里面有一个很大缺陷：每次有新服务加入怎么改 Nginx 配置？总不能手动改或者来个 Rolling Update 前端 Nginx Pod 吧？这时候 “伟大而又正直勇敢的” Ingress 登场，如果不算上面的 Nginx，Ingress 只有两大组件：Ingress Controller 和 IngressIngress 这个玩意，简单的理解就是 你原来要改 Nginx 配置，然后配置各种域名对应哪个 Service，现在把这个动作抽象出来，变成一个 Ingress 对象，你可以用 yml 创建，每次不要去改 Nginx 了，直接改 yml 然后创建/更新就行了；那么问题来了：”Nginx 咋整？”Ingress Controller 这东西就是解决 “Nginx 咋整” 的；Ingress Controoler 通过与 Kubernetes API 交互，动态的去感知集群中 Ingress 规则变化，然后读取他，按照他自己模板生成一段 Nginx 配置，再写到 Nginx Pod 里，最后 reload 一下，工作流程如下图当然在实际应用中，最新版本 Kubernetes 已经将 Nginx 与 Ingress Controller 合并为一个组件，所以 Nginx 无需单独部署，只需要部署 Ingress Controller 即可。 Nginx Ingress部署默认后端我们知道 前端的 Nginx 最终要负载到后端 service 上，那么如果访问不存在的域名咋整？官方给出的建议是部署一个 默认后端，对于未知请求全部负载到这个默认后端上；这个后端啥也不干，就是返回 404， service “default-http-backend” created这个 default-backend.yaml 文件可以在 官方 Ingress 仓库 找到。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: default-http-backend labels: k8s-app: default-http-backend namespace: kube-systemspec: replicas: 1 template: metadata: labels: k8s-app: default-http-backend spec: terminationGracePeriodSeconds: 60 containers: - name: default-http-backend # Any image is permissable as long as: # 1. It serves a 404 page at / # 2. It serves 200 on a /healthz endpoint image: 192.168.204.66/google_containers/defaultbackend:1.0 livenessProbe: httpGet: path: /healthz port: 8080 scheme: HTTP initialDelaySeconds: 30 timeoutSeconds: 5 ports: - containerPort: 8080 resources: limits: cpu: 10m memory: 20Mi requests: cpu: 10m memory: 20Mi---apiVersion: v1kind: Servicemetadata: name: default-http-backend namespace: kube-system labels: k8s-app: default-http-backendspec: ports: - port: 80 targetPort: 8080 selector: k8s-app: default-http-backend 部署 Ingress Controller部署完了后端就得把最重要的组件 Nginx+Ingres Controller(官方统一称为 Ingress Controller) 部署上 注意：官方的 Ingress Controller 有个坑，至少我看了 DaemonSet 方式部署的有这个问题：没有绑定到宿主机 80 端口，也就是说前端 Nginx 没有监听宿主机 80 端口(这还玩个卵啊)；所以需要把配置搞下来自己加一下 hostNetwork 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: nginx-ingress-lb labels: name: nginx-ingress-lb namespace: kube-systemspec: template: metadata: labels: name: nginx-ingress-lb # annotations: # prometheus.io/port: '10254' # prometheus.io/scrape: 'true' spec: terminationGracePeriodSeconds: 60 hostNetwork: true containers: - image: 192.168.204.66/google_containers/nginx-ingress-controller:0.9.0-beta.3 name: nginx-ingress-lb readinessProbe: httpGet: path: /healthz port: 80 scheme: HTTP livenessProbe: httpGet: path: /healthz port: 80 scheme: HTTP initialDelaySeconds: 10 timeoutSeconds: 1 ports: - containerPort: 80 hostPort: 80 - containerPort: 443 hostPort: 443 env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace args: - /nginx-ingress-controller - --default-backend-service=kube-system/default-http-backend - --apiserver-host=https://10.200.102.93:6443 - --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml volumeMounts: - mountPath: /etc/kubernetes/ssl name: ssl-certs-kubernetes - mountPath: /etc/ssl/certs name: ssl-certs-host - mountPath: /etc/kubernetes/worker-kubeconfig.yaml name: config volumes: - name: etcd-storage emptyDir: &#123;&#125; - hostPath: path: /etc/kubernetes/ssl name: ssl-certs-kubernetes - hostPath: path: /etc/pki/tls/certs name: ssl-certs-host - hostPath: path: /etc/kubernetes/worker-kubeconfig.yaml name: config 部署 Ingress从上面可以知道 Ingress 就是个规则，指定哪个域名转发到哪个 Service，所以说首先我们得有个 Service，当然 Service 去哪找这里就不管了；这里默认为已经有了两个可用的 Service，以下以 Dashboard 和 kibana 为例先写一个 Ingress 文件，语法格式啥的请参考官方文档，由于我的 Dashboard 和 Kibana 都在 kube-system 这个命名空间，所以要指定 namespace，写之前 Service 分布如下 12345678910111213apiVersion: extensions/v1beta1kind: Ingressmetadata: name: dashboard-ingress namespace: kube-systemspec: rules: - host: dashboard.candao.jerome http: paths: - backend: serviceName: kubernetes-dashboard servicePort: 80]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Kubernetes 配置HPA，实现动态扩容]]></title>
      <url>%2F2017%2F04%2F24%2FKubernetes-%E9%85%8D%E7%BD%AEHPA%EF%BC%8C%E5%AE%9E%E7%8E%B0%E5%8A%A8%E6%80%81%E6%89%A9%E5%AE%B9%2F</url>
      <content type="text"><![CDATA[优秀的人，不是不合群，而是他们合群的人里面没有你 测试方案一(CPU限制)下载必要的yaml文件https://github.com/kubernetes/kubernetes/tree/8caeec429ee1d2a9df7b7a41b21c626346b456fb/docs/user-guide/horizontal-pod-autoscaling 修改官方的规则官方的方案是基于RC的，现在我们更换成Deployment的方式。 cat php-apache-Deployment.yaml cat php-apache-Service.yaml cat php-apache-Hpa.yaml 创建Podkubectl create -f /home/kube-yaml/pod-autoscaler/ 查看集群情况 注：k8s是基于周期性采样进行处理扩容和收缩的。所以，并不是根据cup或者memory的变化，立马做出扩容的处理(需要几分钟的时间)。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Kubernetes 搭建grafana+heapster+influxdb监控服务]]></title>
      <url>%2F2017%2F04%2F24%2FKubernetes-%E6%90%AD%E5%BB%BAgrafana-heapster-influxdb%E7%9B%91%E6%8E%A7%E6%9C%8D%E5%8A%A1%2F</url>
      <content type="text"><![CDATA[优秀的人，不是不合群，而是他们合群的人里面没有你 下载必要的yaml文件到github上面进行下载deploy/kube-config/influxdb 修改Yaml文件信息替换镜像来源12345678gcr.io/google_containers/heapster-grafana:v4.0.2registry.cn-hangzhou.aliyuncs.com/kube_containers/heapster_grafanagcr.io/google_containers/heapster-influxdb:v1.1.1registry.cn-hangzhou.aliyuncs.com/kube_containers/heapster_influxdbgcr.io/google_containers/heapster:v1.3.0-beta.0registry.cn-hangzhou.aliyuncs.com/wayne/heapster:v1.1.0 替换内容部分cat grafana-deployment.yaml cat grafana-service.yaml cat heapster-deployment.yaml cat /etc/kubernetes/test-kubeconfig.yaml cat heapster-service.yaml cat influxdb-deployment.yaml cat influxdb-service.yaml 创建podkubectl create -f /home/kube-yaml/grafana-heapster-influxdb 查看镜像运行情况 查看界面]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Kubernetes 搭建jenkins]]></title>
      <url>%2F2017%2F04%2F24%2FKubernetes-%E6%90%AD%E5%BB%BAjenkins%2F</url>
      <content type="text"><![CDATA[优秀的人，不是不合群，而是他们合群的人里面没有你 创建项目 配置信息 配置编译和生成镜像文件 启动界面 运行结果]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Kubernetes 搭建kube-dns]]></title>
      <url>%2F2017%2F04%2F24%2FKubernetes-%E6%90%AD%E5%BB%BAkube-dns%2F</url>
      <content type="text"><![CDATA[优秀的人，不是不合群，而是他们合群的人里面没有你 第一种方案(容器部署方案)拉取官方kube-dns yaml文件wget https://rawgit.com/kubernetes/kubernetes/release-1.5/cluster/addons/dns/skydns-rc.yaml.sed -O skydns-rc.yaml wget https://rawgit.com/kubernetes/kubernetes/release-1.5/cluster/addons/dns/skydns-svc.yaml.sed -O skydns-svc.yaml 编辑yaml文件编辑 skydns-rc.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197# Copyright 2016 The Kubernetes Authors.## Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# TODO - At some point, we need to rename all skydns-*.yaml.* files to kubedns-*.yaml.*# Should keep target in cluster/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml# in sync with this file.# Warning: This is a file generated from the base underscore template file: skydns-rc.yaml.baseapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: kube-dns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: "true"spec: #指定副本数 replicas: 1 # replicas: not specified here: # 1. In order to make Addon Manager do not reconcile this replicas parameter. # 2. Default is 1. # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on. strategy: rollingUpdate: maxSurge: 10% maxUnavailable: 0 selector: matchLabels: k8s-app: kube-dns template: metadata: labels: k8s-app: kube-dns annotations: scheduler.alpha.kubernetes.io/critical-pod: '' scheduler.alpha.kubernetes.io/tolerations: '[&#123;"key":"CriticalAddonsOnly", "operator":"Exists"&#125;]' spec: containers: - name: kubedns image: 192.168.204.66/google_containers/kubedns-amd64:1.9 resources: # TODO: Set memory limits when we've profiled the container for large # clusters, then set request = limit to keep this container in # guaranteed class. Currently, this container falls into the # "burstable" category so the kubelet doesn't backoff from restarting it. limits: memory: 170Mi requests: cpu: 100m memory: 70Mi livenessProbe: httpGet: path: /healthz-kubedns port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 readinessProbe: httpGet: path: /readiness port: 8081 scheme: HTTP # we poll on pod startup for the Kubernetes master service and # only setup the /readiness HTTP server once that's available. initialDelaySeconds: 3 timeoutSeconds: 5 args: # --domain指定一级域名，可自定义 # - --domain=$DNS_DOMAIN. - --domain=cluster.local #设置k8s集群中Service所属的域名 - --dns-port=10053 - --config-map=kube-dns # 增加--kube-master-url，指向kube master的地址 - --kube-master-url=https://10.200.102.93:6443 #k8s中master的ip地址和apiserver中配置的端口号 - --kubecfg_file=/etc/kubernetes/worker-kubeconfig.yaml # This should be set to v=2 only after the new image (cut from 1.5) has # been released, otherwise we will flood the logs. - --v=0 volumeMounts: - mountPath: /etc/kubernetes/ssl name: ssl-certs-kubernetes - mountPath: /etc/ssl/certs name: ssl-certs-host - mountPath: /etc/kubernetes/worker-kubeconfig.yaml name: config env: - name: PROMETHEUS_PORT value: "10055" ports: - containerPort: 10053 name: dns-local protocol: UDP - containerPort: 10053 name: dns-tcp-local protocol: TCP - containerPort: 10055 name: metrics protocol: TCP - name: dnsmasq image: 192.168.204.66/google_containers/kube-dnsmasq-amd64:1.4 livenessProbe: httpGet: path: /healthz-dnsmasq port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 args: - --cache-size=1000 - --no-resolv - --server=127.0.0.1#10053 # - --log-facility=- ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP # see: https://github.com/kubernetes/kubernetes/issues/29055 for details resources: requests: cpu: 150m memory: 10Mi - name: dnsmasq-metrics image: 192.168.204.66/google_containers/dnsmasq-metrics-amd64:1.0 livenessProbe: httpGet: path: /metrics port: 10054 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 args: - --v=2 - --logtostderr ports: - containerPort: 10054 name: metrics protocol: TCP resources: requests: memory: 10Mi - name: healthz image: 192.168.204.66/google_containers/exechealthz-amd64:1.2 resources: limits: memory: 50Mi requests: cpu: 10m # Note that this container shouldn't really need 50Mi of memory. The # limits are set higher than expected pending investigation on #29688. # The extra memory was stolen from the kubedns container to keep the # net memory requested by the pod constant. memory: 50Mi args: - --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1 &gt;/dev/null - --url=/healthz-dnsmasq - --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1:10053 &gt;/dev/null - --url=/healthz-kubedns - --port=8080 - --quiet ports: - containerPort: 8080 protocol: TCP dnsPolicy: Default # Don't use cluster DNS. volumes: - name: etcd-storage emptyDir: &#123;&#125; - hostPath: path: /etc/kubernetes/ssl name: ssl-certs-kubernetes - hostPath: path: /etc/pki/tls/certs name: ssl-certs-host - hostPath: path: /etc/kubernetes/worker-kubeconfig.yaml name: config 编辑skydns-svc.yaml文件1234567891011121314151617181920212223242526272829303132333435363738[root@k8s-master kube-yaml]# cat sky-dns/skydns-svc.yaml # Copyright 2016 The Kubernetes Authors.## Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# TODO - At some point, we need to rename all skydns-*.yaml.* files to kubedns-*.yaml.*# Warning: This is a file generated from the base underscore template file: skydns-svc.yaml.baseapiVersion: v1kind: Servicemetadata: name: kube-dns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: "true" kubernetes.io/name: "KubeDNS"spec: selector: k8s-app: kube-dns clusterIP: 172.30.0.3 #/etc/kubernetes/kubelet中已经设定好clusterIP ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 运行yaml文件123[root@seanzhau.com ~]# kubectl create -f skydns-rc.yaml[root@seanzhau.com ~]# kubectl create -f skydns-svc.yaml[root@seanzhau.com ~]# kubectl create -f busybox.yaml 运行结果 第二种方案(插件/命令部署方案)该方案目前尚未寻找到支持TLS的方案实现，但木有已经实现了无数字证书验证的方案。 下载kube-dns命令123# wget https://dl.k8s.io/v1.5.2/kubernetes-server-linux-amd64.tar.gz# tar -xf kubernetes-server-linux-amd64.tar.gz# mv /opt/docker/src/kubernetes/server/bin/kube-dns /usr/bin/ 新建kube-dns配置文件12345# vi /etc/kubernetes/kube-dnsKUBE_DNS_PORT="--dns-port=53"KUBE_DNS_DOMAIN="--domain=cluster.local"KUBE_DNS_MASTER=--kube-master-url="http://192.168.40.50:8080"KUBE_DNS_ARGS="" 新建kube-dns.service配置文件123456789101112131415161718[root@k8s-master calico]# cat /usr/lib/systemd/system/kube-dns.service [Unit]Description=Kubernetes Kube-dns ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=kube-apiserver.serviceRequires=kube-apiserver.service [Service]WorkingDirectory=/var/lib/kube-dnsEnvironmentFile=-/etc/kubernetes/kube-dnsExecStart=/usr/bin/kube-dns \ $KUBE_DNS_PORT \ $KUBE_DNS_DOMAIN \ $KUBE_DNS_MASTER \ $KUBE_DNS_ARGSRestart=on-failure[Install]WantedBy=multi-user.target master启动123# mkdir -p /var/lib/kube-dns# systemctl enable kube-dns# systemctl restart kube-dns master修改/etc/resolv.conf文件123456[root@k8s-master calico]# cat /etc/resolv.conf# Generated by NetworkManagersearch default.svc.cluster.local svc.cluster.local cluster.localnameserver 10.200.102.95nameserver 223.5.5.5nameserver 202.96.128.86 node结点修改kubelet文件1234567891011121314151617181920[root@k8s-node-1 ~]# cat /etc/kubernetes/kubelet #### kubernetes kubelet (minion) config# The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)#KUBELET_ADDRESS="--address=127.0.0.1"KUBELET_ADDRESS="--address=0.0.0.0"# The port for the info server to serve on# KUBELET_PORT="--port=10250"# You may leave this blank to use the actual hostname#KUBELET_HOSTNAME="--hostname-override=127.0.0.1"KUBELET_HOSTNAME="--hostname-override=k8s-node-1"# location of the api-server#KUBELET_API_SERVER="--api-servers=http://127.0.0.1:8080"KUBELET_API_SERVER="--api-servers=http://10.200.102.95:8080"# pod infrastructure container#KUBELET_POD_INFRA_CONTAINER="--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest"KUBELET_POD_INFRA_CONTAINER="--pod-infra-container-image=192.168.204.66/google_containers/pause:latest"# Add your own!KUBELET_ARGS="--cluster-dns=10.200.102.95 --cluster-domain=cluster.local" 检测123456[root@k8s-master kube]# kubectl exec -ti busybox --namespace=kube-system -- nslookup kubernetes.defaultServer: 10.200.102.95Address 1: 10.200.102.95Name: kubernetes.defaultAddress 1: 10.254.0.1 kubernetes.default.svc.cluster.local]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Kubernetes 搭建dashboard]]></title>
      <url>%2F2017%2F04%2F21%2FKubernetes-%E6%90%AD%E5%BB%BAdashboard%2F</url>
      <content type="text"><![CDATA[优秀的人，不是不合群，而是他们合群的人里面没有你 拉去官方的rc文件(Deployment)相关的拉取路径：kubectl create -f https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253kind: DeploymentapiVersion: extensions/v1beta1metadata: labels: app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: replicas: 1 selector: matchLabels: app: kubernetes-dashboard template: metadata: labels: app: kubernetes-dashboard spec: containers: - name: kubernetes-dashboard image: index.tenxcloud.com/google_containers/kubernetes-dashboard-amd64:v1.4.1 imagePullPolicy: IfNotPresent ports: - containerPort: 9090 protocol: TCP args: # Uncomment the following line to manually specify Kubernetes API server Host # If not specified, Dashboard will attempt to auto discover the API server and connect # to it. Uncomment only if the default does not work. - --apiserver-host=https://10.200.102.93:6443 - --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml livenessProbe: httpGet: path: / port: 9090 initialDelaySeconds: 30 timeoutSeconds: 30 volumeMounts: - mountPath: /etc/kubernetes/ssl name: ssl-certs-kubernetes - mountPath: /etc/ssl/certs name: ssl-certs-host - mountPath: /etc/kubernetes/worker-kubeconfig.yaml name: config volumes: - hostPath: path: /etc/kubernetes/ssl name: ssl-certs-kubernetes - hostPath: path: /etc/pki/tls/certs name: ssl-certs-host - hostPath: path: /etc/kubernetes/worker-kubeconfig.yaml name: config 对应的Service文件123456789101112131415kind: ServiceapiVersion: v1metadata: labels: app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: type: NodePort ports: - port: 80 nodePort: 30010 targetPort: 9090 selector: app: kubernetes-dashboard 执行rc文件1kubectl create -f kubernetes-dashboard.yaml 查看pod状态 访问地址：nodeIP：NodePort 界面]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Kubernetes 双向证书TLS配置]]></title>
      <url>%2F2017%2F04%2F21%2FKubernetes-%E5%8F%8C%E5%90%91%E8%AF%81%E4%B9%A6TLS%E9%85%8D%E7%BD%AE%2F</url>
      <content type="text"><![CDATA[优秀的人，不是不合群，而是他们合群的人里面没有你 签发证书TLS双向认知需要预先自建CA签发证书，权威CA机构的证书应该不可用，因为大部分k8s都是在内网中部署，而内网应该都会采用私有IP地址通讯，权威CA好像只能签署域名证书，对签署到IP可能无法实现. 自签CA对于私有证书签发首先要自签署一个CA根证书创建证书存放的目录,创建CA私钥,自签CA12345[root@master ~]# mkdir /etc/kubernetes/ssl &amp;&amp; cd /etc/kubernetes/sslopenssl genrsa -out ca-key.pem 2048[root@master ssl]# openssl req -x509 -new -nodes -key ca-key.pem -days 10000 -out ca.pem -subj "/CN=kube-ca"[root@master ssl]# lsca-key.pem ca.pem 签署apiserver 证书自签 CA 后就需要使用这个根 CA 签署 apiserver 相关的证书了，首先先修改 openssl 的配置。12345678910111213141516# vim openssl.cnf[req]req_extensions = v3_reqdistinguished_name = req_distinguished_name[req_distinguished_name][ v3_req ]basicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEnciphermentsubjectAltName = @alt_names[alt_names]DNS.1 = kubernetesDNS.2 = kubernetes.defaultDNS.3 = kubernetes.default.svcDNS.4 = kubernetes.default.svc.cluster.localIP.1 = 10.254.0.1 #k8s 集群service ip(dns),关键地方IP.2 = 10.200.102.93 #k8s master ip 然后开始签署apiserver相关的证书123456# 生成 apiserver 私钥openssl genrsa -out apiserver-key.pem 2048# 生成签署请求openssl req -new -key apiserver-key.pem -out apiserver.csr -subj "/CN=kube-apiserver" -config openssl.cnf# 使用自建 CA 签署openssl x509 -req -in apiserver.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out apiserver.pem -days 365 -extensions v3_req -extfile openssl.cnf 生成集群管理证书123openssl genrsa -out admin-key.pem 2048openssl req -new -key admin-key.pem -out admin.csr -subj "/CN=kube-admin"openssl x509 -req -in admin.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out admin.pem -days 365 签署node证书1234567891011121314[root@master ssl]# cp openssl.cnf worker-openssl.cnf[root@master ssl]# cat worker-openssl.cnf [req]req_extensions = v3_reqdistinguished_name = req_distinguished_name[req_distinguished_name][ v3_req ]basicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEnciphermentsubjectAltName = @alt_names[alt_names]IP.1 = 10.200.102.92IP.2 = 10.200.102.81IP.3 = 10.200.102.82 生成各个结点的证书并且拷贝到每个节点的目录下12345678[root@master ssl]# for i in &#123;k8s-master,k8s-node-1,k8s-node-2,k8s-node-3&#125;&gt; do&gt; openssl genrsa -out $i-worker-key.pem 2048&gt; openssl req -new -key $i-worker-key.pem -out $i-worker.csr -subj "/CN=$i" -config worker-openssl.cnf&gt; openssl x509 -req -in $i-worker.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out $i-worker.pem -days 365 -extensions v3_req -extfile worker-openssl.cnf&gt; ssh root@$i "mkdir /etc/kubernetes/ssl;chown kube:kube -R /etc/kubernetes/ssl"&gt; scp /etc/kubernetes/ssl/ca.pem /etc/kubernetes/ssl/$i* root@$i:/etc/kubernetes/ssl&gt; done 配置k8s配置masterapiserver文件123456789101112131415161718192021KUBE_API_ADDRESS="--bind-address=10.200.102.93 --insecure-bind-address=127.0.0.1 "# The port on the local server to listen on.KUBE_API_PORT=="--secure-port=6443 --insecure-port=8080"# Port minions listen on# KUBELET_PORT="--kubelet-port=10250"# Comma separated list of nodes in the etcd clusterKUBE_ETCD_SERVERS="--etcd-servers=http://10.200.102.93:2379"# Address range to use for servicesKUBE_SERVICE_ADDRESSES="--service-cluster-ip-range=10.254.0.0/16"# default admission control policiesKUBE_ADMISSION_CONTROL="--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota"# Add your own!KUBE_API_ARGS="--tls-cert-file=/etc/kubernetes/ssl/apiserver.pem \ --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem \ --client-ca-file=/etc/kubernetes/ssl/ca.pem \ --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem" config文件1234567891011121314151617181920212223#### kubernetes system config## The following values are used to configure various aspects of all# kubernetes services, including## kube-apiserver.service# kube-controller-manager.service# kube-scheduler.service# kubelet.service# kube-proxy.service# logging to stderr means we get it in the systemd journalKUBE_LOGTOSTDERR="--logtostderr=true"# journal message level, 0 is debugKUBE_LOG_LEVEL="--v=0"# Should this cluster be allowed to run privileged docker containersKUBE_ALLOW_PRIV="--allow-privileged=false"# How the controller-manager, scheduler, and proxy find the apiserver#KUBE_MASTER="--master=http://k8s-master:8080"KUBE_MASTER="--master=https://k8s-master:6443" scheduler文件12345678[root@k8s-master kube-yaml]# cat /etc/kubernetes/scheduler #### kubernetes scheduler config# default config should be adequate# Add your own!KUBE_SCHEDULER_ARGS="--kubeconfig=/etc/kubernetes/cm-kubeconfig.yaml --master=http://127.0.0.1:8080" controller-manager文件123456789[root@k8s-master kube-yaml]# cat /etc/kubernetes/controller-manager #### The following values are used to configure the kubernetes controller-manager# defaults from config and apiserver should be adequate# Add your own!KUBE_CONTROLLER_MANAGER_ARGS="--service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem \ --root-ca-file=/etc/kubernetes/ssl/ca.pem \ --master=http://127.0.0.1:8080 \ --kubeconfig=/etc/kubernetes/cm-kubeconfig.yaml" 创建一个/etc/kubernetes/cm-kubeconfig.yaml 文件1234567891011121314151617apiVersion: v1kind: Configclusters:- name: local cluster: certificate-authority: /etc/kubernetes/ssl/ca.pemusers:- name: controllermanager user: client-certificate: /etc/kubernetes/ssl/apiserver.pem client-key: /etc/kubernetes/ssl/apiserver-key.pemcontexts:- context: cluster: local user: controllermanager name: kubelet-contextcurrent-context: kubelet-context 重启服务1systemctl restart etcd kube-apiserver.service kube-controller-manager.service kube-scheduler.service 配置node结点（以node1为例子）config文件123456789101112131415161718192021222324[root@k8s-node-1 ~]# cat /etc/kubernetes/config #### kubernetes system config## The following values are used to configure various aspects of all# kubernetes services, including## kube-apiserver.service# kube-controller-manager.service# kube-scheduler.service# kubelet.service# kube-proxy.service# logging to stderr means we get it in the systemd journalKUBE_LOGTOSTDERR="--logtostderr=true"# journal message level, 0 is debugKUBE_LOG_LEVEL="--v=0"# Should this cluster be allowed to run privileged docker containersKUBE_ALLOW_PRIV="--allow-privileged=false"# How the controller-manager, scheduler, and proxy find the apiserver#KUBE_MASTER="--master=http://127.0.0.1:8080"KUBE_MASTER="--master=https://10.200.102.93:6443" kubelet文件12345678910111213141516171819202122232425262728[root@k8s-node-1 ~]# cat /etc/kubernetes/kubelet#### kubernetes kubelet (minion) config# The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)#KUBELET_ADDRESS="--address=0.0.0.0"KUBELET_ADDRESS="--address=10.200.102.92"# The port for the info server to serve onKUBELET_PORT="--port=10250"# You may leave this blank to use the actual hostnameKUBELET_HOSTNAME="--hostname-override=k8s-node-1"# location of the api-server#KUBELET_API_SERVER="--api-servers=http://k8s-master:8080"KUBELET_API_SERVER="--api-servers=https://10.200.102.93:6443"# pod infrastructure container#KUBELET_POD_INFRA_CONTAINER="--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest"KUBELET_POD_INFRA_CONTAINER="--pod-infra-container-image=192.168.204.66/google_containers/pause:latest"# Add your own!KUBELET_ARGS="--cluster_dns=172.30.0.3 \ --cluster_domain=cluster.local \ --tls-cert-file=/etc/kubernetes/ssl/k8s-node-1-worker.pem \ --tls-private-key-file=/etc/kubernetes/ssl/k8s-node-1-worker-key.pem \ --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml \ --allow-privileged=true " proxy文件12345678[root@k8s-node-1 ~]# cat /etc/kubernetes/proxy #### kubernetes proxy config# default config should be adequate# Add your own!KUBE_PROXY_ARGS="--proxy-mode=iptables \ --master=https://10.200.102.93:6443 \ --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml" 创建一个文件worker-kubeconfig.yaml123456789101112131415161718apiVersion: v1kind: Configclusters:- name: local cluster: server: https://10.200.102.93:6443 certificate-authority: /etc/kubernetes/ssl/ca.pemusers:- name: kubelet user: client-certificate: /etc/kubernetes/ssl/node1-worker.pem client-key: /etc/kubernetes/ssl/node1-worker-key.pemcontexts:- context: cluster: local user: kubelet name: kubelet-contextcurrent-context: kubelet-context 123456node1#重启服务systemctl restart kubelet kube-proxy#查看 状态systemctl status kubelet kube-proxy -l 验证TLS12#验证证书curl https://10.200.102.93:6443/api/v1/nodes --cert /etc/kubernetes/ssl/k8s-node-1-worker.pem --key /etc/kubernetes/ssl/k8s-node-1-worker-key.pem --cacert /etc/kubernetes/ssl/ca.pem]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Kubernetes环境搭建]]></title>
      <url>%2F2017%2F04%2F20%2FKubernetes%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
      <content type="text"><![CDATA[优秀的人，不是不合群，而是他们合群的人里面没有你 第一种安装方案(官网)基于官方的安装方式(安装包并非是最新版本的) 准备CentOS 7.x环境查看内核版本123[root@k8s-master kube-yaml]# uname -r3.10.0-514.6.1.el7.x86_64[root@k8s-master kube-yaml]# 最好是选择3.10版本以上的内核，进行安装。本次安装，选择了4台服务器进行集群安装。 123456[root@k8s-master kube-yaml]# cat /etc/hosts10.200.102.93 k8s-master10.200.102.92 k8s-node-110.200.102.81 k8s-node-210.200.102.82 k8s-node-3[root@k8s-master kube-yaml]# 配置官方k8s yum源:1234[virt7-docker-common-release]name=virt7-docker-common-releasebaseurl=http://cbs.centos.org/repos/virt7-docker-common-release/x86_64/os/gpgcheck=0 配置阿里云yum源:123456789101112131415161718192021222324252627282930313233343536373839404142434445[base]name=CentOS-$releasever - Base - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos/$releasever/os/$basearch/ http://mirrors.aliyuncs.com/centos/$releasever/os/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=osgpgcheck=1gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7#released updates[updates]name=CentOS-$releasever - Updates - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos/$releasever/updates/$basearch/ http://mirrors.aliyuncs.com/centos/$releasever/updates/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=updatesgpgcheck=1gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7#additional packages that may be useful[extras]name=CentOS-$releasever - Extras - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos/$releasever/extras/$basearch/http://mirrors.aliyuncs.com/centos/$releasever/extras/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=extrasgpgcheck=1gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7#additional packages that extend functionality of existing packages[centosplus]name=CentOS-$releasever - Plus - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos/$releasever/centosplus/$basearch/ http://mirrors.aliyuncs.com/centos/$releasever/centosplus/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=centosplusgpgcheck=1enabled=0gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7#contrib - packages by Centos Users[contrib]name=CentOS-$releasever - Contrib - mirrors.aliyun.comfailovermethod=prioritybaseurl=http://mirrors.aliyun.com/centos/$releasever/contrib/$basearch/http://mirrors.aliyuncs.com/centos/$releasever/contrib/$basearch/#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=contribgpgcheck=1enabled=0gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7 更新本地镜像源12yum clean allyum makecache 安装Kubernetes环境(Master)1yum -y install --enablerepo=virt7-docker-common-release kubernetes etcd flannel安装的过程有点久，因为需要下载和安装。期间如果出现什么下载失败，更新包更新失败。基本上都是因为yum的问题，换个国内大企业的镜像yum就好了。至此，整个下载和安装的过程就算成功了。编辑本地host文件，做好访问映射：vim /etc/hosts1234567[root@k8s-master kube-yaml]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain610.200.102.93 k8s-master10.200.102.92 k8s-node-110.200.102.81 k8s-node-210.200.102.82 k8s-node-3编辑k8s的配置文件信息：vi /etc/kubernetes/config由于CentOS 7.x默认是开启防火墙的，需要进行防火墙的设置操作：关闭SELinux：setenforce 01、临时关闭(不用重启机器):setenforce 0 #设置SELinux 成为permissive模式setenforce 1 #设置SELinux 成为enforcing模式2、关闭防火墙:12systemctl stop firewalld.servicesystemctl disable firewalld.service 编辑etcd的配置文件信息：vi /etc/etcd/etcd.conf 编辑k8s的配置信息：vi /etc/kubernetes/apiserver 启动etcd服务：systemctl start etcd 创建网络，并且设置网络配置信息：123etcdctl mkdir /kube-centos/networketcdctl mk /kube-centos/network/config "&#123;\"Network\":\"172.30.0.0/16\",\"SubnetLen\":24,\"Backend\":&#123;\"Type\":\"vxlan\"&#125;&#125;" 配置flanneld信息：vi /etc/sysconfig/flanneld 运行环境：for SERVICES in etcd kube-apiserver kube-controller-manager kube-scheduler flanneld; do systemctl restart $SERVICES; systemctl enable $SERVICES; systemctl status $SERVICES; done 至此k8s-master的安装和启动到此完成。 安装Kubernetes环境(Minion/Node):内核版本和yum源配置，请参考上面部分进行配置就可以了。1yum -y install --enablerepo=virt7-docker-common-release kubernetes flannel 安装的过程有点久，因为需要下载和安装。期间如果出现什么下载失败，更新包更新失败。基本上都是因为yum的问题，换个国内大企业的镜像yum就好了。至此，整个下载和安装的过程就算成功了。 编辑本地host文件，做好访问映射：vim /etc/hosts1234567[root@k8s-master kube-yaml]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain610.200.102.93 k8s-master10.200.102.92 k8s-node-110.200.102.81 k8s-node-210.200.102.82 k8s-node-3 由于CentOS 7.x默认是开启防火墙的，需要进行防火墙的设置操作：关闭SELinux：setenforce 01、临时关闭(不用重启机器): setenforce 0 #设置SELinux 成为permissive模式setenforce 1 #设置SELinux 成为enforcing模式2、关闭防火墙:12systemctl stop firewalld.servicesystemctl disable firewalld.service 编辑k8s的配置文件信息：vi /etc/kubernetes/config 配置kubernetes信息：vi /etc/kubernetes/kubelet 配置flanneld信息：vi /etc/sysconfig/flanneld 运行环境：for SERVICES in kube-proxy kubelet flanneld docker; do systemctl restart $SERVICES; systemctl enable $SERVICES; systemctl status $SERVICES; done 配置参数：123kubectl config set-cluster default-cluster --server=http://k8s-master:8080kubectl config set-context default-context --cluster=default-cluster --user=default-adminkubectl config use-context default-context 第二种安装方案(tar安装)服务器环境：123456710.15.206.120 vip 10.15.206.105 master 10.15.206.106 node 10.15.206.107 etcd1 node 10.15.206.108 etcd2 node 10.15.206.109 etcd3 第一步：配置flannel网卡,先在etcd中注册flannel子网：1etcdctl set /coreos.com/network/config '&#123;"network": "172.16.0.0/16"&#125;' 第二步：在所有节点安装flannel1yum install -y flannel 第三步：修改flannel配置文件/etc/sysconfig/flanneld12FLANNEL_ETCD="http://10.15.206.107:2379,http://10.15.206.108:2379,http://10.15.206.109:2379" FLANNEL_ETCD_KEY="/coreos.com/network" 重启flannel：12systemctl start flanneld systemctl enable flanneld 需要说明的是，如果要让docker使用flannel的网络，docker必须要后于flannel启动，所以需要重新启动docker1systemctl restart docker 第四步：下载地址kubernetes-client地址https://storage.googleapis.com/kubernetes-release/release/v1.5.3/kubernetes-client-linux-amd64.tar.gz kubernetes-server地址：https://storage.googleapis.com/kubernetes-release/release/v1.5.3/kubernetes-server-linux-amd64.tar.gz 第五步：在server端服务器解压包tar zxvf kubernetes-server-linux-amd64.tar.gz cd kubernetes/server/bin 然后将文件复制到/usr/local/bin下1234for i in `ls -F|grep "*"|awk '&#123;print $1&#125;'|awk -F "*" '&#123;print $1&#125;'`;do cp $i /usr/local/bin/ ;done 第六步：启动master启动api-server12345678910kube-apiserver --address=0.0.0.0 --insecure-port=8080 --service-cluster-ip-range='10.15.206.120/24' --log_dir=/usr/local/kubernetes/logs/kube --kubelet_port=10250 --v=0 --logtostderr=false --etcd_servers=http://10.15.206.107:2379,http://10.15.206.108:2379,http://10.15.206.109:2379 --allow_privileged=false &gt;&gt; /usr/local/kubernetes/logs/kube-apiserver.log 2&gt;&amp;1 &amp; 启动controller-manager12345kube-controller-manager --v=0 --logtostderr=false --log_dir=/usr/local/kubernetes/logs/kube --master=10.15.206.120:8080 &gt;&gt; /usr/local/kubernetes/logs/kube-controller-manager 2&gt;&amp;1 &amp; 启动scheduler1234kube-scheduler --master='10.15.206.120:8080' --v=0 --log_dir=/usr/local/kubernetes/logs/kube &gt;&gt; /usr/local/kubernetes/logs/kube-scheduler.log 2&gt;&amp;1 &amp; 第七步：验证是否成功1234567kubectl get componentstatuses NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy &#123;"health": "true"&#125; etcd-0 Healthy &#123;"health": "true"&#125; etcd-1 Healthy &#123;"health": "true"&#125; 第八步：配置client123tar zxvf kubernetes-client-linux-amd64.tar.gz cd kubernetes/client/bin cp * /usr/local/bin/ 第九步：启动client启动kubelet123456789kubelet --logtostderr=false --v=0 --allow-privileged=false --log_dir=/usr/local/kubernetes/logs/kube --address=0.0.0.0 --port=10250 --hostname_override=10.15.206.120 --api_servers=http://10.15.206.120:8080 &gt;&gt; /usr/local/kubernetes/logs/kube-kubelet.log 2&gt;&amp;1 &amp; 启动proxy1234kube-proxy --logtostderr=false --v=0 --master=http://10.15.206.120 第三种安装方案(calico)环境介绍: 服务器 Ip Hosts Centos-7.3 10.200.102.95 k8s-master Centos-7.3 10.200.102.94 k8s-node-1 Centos-7.3 10.200.102.85 k8s-node-2 Centos-7.3 10.200.102.90 k8s-node-3 确保操作系统的内核是3.10版本以上的。并且关闭防火墙和selinux。123setenforce 0systemctl stop firewalld.servicesystemctl disable firewalld.service 根据需要是否配置必要的源，可以参考上述的源配置。 etc环境安装(可以选择集群的方案安装)服务器 IP Hosts| 服务器 | Ip | Hosts || ————- |:—————:|:———:|| Centos-7.3 | 10.200.102.85 | Echo0 || Centos-7.3 | 10.200.102.86 | Echo1 || Centos-7.3 | 10.200.102.84 | Echo2 | 安装ectd环境 配置etcd信息 启动服务所有的节点都进行如上相应的配置 安装k8s master环境1yum install kubernetes-master docker -y 配置好相应的kubernetes信息 配置好docker信息 查看集群信息 安装k8s node环境1yum install kubernetes-node docker –y 配置k8s和docker信息配置kubectl配置proxy配置config配置docker镜像拉取位置 查看集群信息 安装kube-dns环境(master节点)1234567891011下载kube-dns命令# wget https://dl.k8s.io/v1.5.2/kubernetes-server-linux-amd64.tar.gz# tar -xf kubernetes-server-linux-amd64.tar.gz# mv /opt/docker/src/kubernetes/server/bin/kube-dns /usr/bin/新建kube-dns配置文件# vi /etc/kubernetes/kube-dnsKUBE_DNS_PORT="--dns-port=53"KUBE_DNS_DOMAIN="--domain=cluster.local"KUBE_DNS_MASTER=--kube-master-url="http://10.200.102.95:8080”KUBE_DNS_ARGS="" 12345678910111213141516171819新建kube-dns.service配置文件# cat /usr/lib/systemd/system/kube-dns.service[Unit]Description=Kubernetes Kube-dns ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=kube-apiserver.serviceRequires=kube-apiserver.service [Service]WorkingDirectory=/var/lib/kube-dnsEnvironmentFile=-/etc/kubernetes/kube-dnsExecStart=/usr/bin/kube-dns \ $KUBE_DNS_PORT \ $KUBE_DNS_DOMAIN \ $KUBE_DNS_MASTER \ $KUBE_DNS_ARGSRestart=on-failure[Install]WantedBy=multi-user.target 123456789101112Master启动# mkdir -p /var/lib/kube-dns# systemctl enable kube-dns# systemctl restart kube-dnsmaster修改/etc/resolv.conf文件# cat /etc/resolv.conf# Generated by NetworkManagersearch default.svc.cluster.local svc.cluster.local cluster.localnameserver 10.200.102.95nameserver 223.5.5.5nameserver 202.96.128.86 node结点修改kubelet文件 验证kube-dns是否安装成功 安装calico环境配置各个节点docker环境： 配置好，记得重启docker12# systemctl daemon-reload# systemctl restart docker 下载calico插件1234567891011121314151617181920Master节点：# wget https://github.com/projectcalico/calicoctl/releases/download/v1.1.0/calicoctl# chmod +x calicoctl# mv calicoctl /usr/bin/# docker pull docker.io/calico/node:v1.1.0# docker tag docker.io/calico/node:v1.1.0 quay.io/calico/node:v1.1.0#wget N -P /opt/cni/bin/ https://github.com/projectcalico/calico-cni/releases/download/v1.6.0/calico# wget -N -P /opt/cni/bin/ https://github.com/projectcalico/calico-cni/releases/download/v1.6.0/calico-ipam# chmod +x /opt/cni/bin/calico /opt/cni/bin/calico-ipamNode节点：# docker pull docker.io/calico/node:v1.1.0# docker tag docker.io/calico/node:v1.1.0 quay.io/calico/node:v1.1.0# wget -N -P /opt/cni/bin/ https://github.com/projectcalico/calico-cni/releases/download/v1.6.0/calico# wget -N -P /opt/cni/bin/ https://github.com/projectcalico/calico-cni/releases/download/v1.6.0/calico-ipam# chmod +x /opt/cni/bin/calico /opt/cni/bin/calico-ipam 配置文件(所有节点) Master机上wget http://docs.projectcalico.org/v1.5/getting-started/kubernetes/installation/policy-controller.yaml 修改 policy-controller.yaml文件里的etcd的地址 启动文件：1234567891011# serivce etcd restart# kubectl create -f policy-controller.yaml每个节点上启动calico-node服务(ETCD_AUTHORITY可以配置多个（集群方案）)# systemctl enable calico-node# systemctl start calico-node# export ETCD_AUTHORITY=10.200.102.85:2379验证calico是否启动正常calicoctl node statuscalicoctl get nodes --out=wide 添加子网 至此calico的k8s方案搭建成功]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java HashMap 源码解析]]></title>
      <url>%2F2017%2F04%2F19%2FJava-HashMap-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%2F</url>
      <content type="text"><![CDATA[优秀的人，不是不合群，而是他们合群的人里面没有你继上一篇文章Java集合框架综述后，今天正式开始分析具体集合类的代码，首先以既熟悉又陌生的HashMap开始。 签名(signature)123public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable 可以看到HashMap继承了 标记接口Cloneable，用于表明HashMap对象会重写java.lang.Object#clone()方法，HashMap实现的是浅拷贝（shallow copy）。 标记接口Serializable，用于表明HashMap对象可以被序列化比较有意思的是，HashMap同时继承了抽象类AbstractMap与接口Map，因为抽象类AbstractMap的签名为123public abstract class AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt; Stack Overfloooow上解释到：在语法层面继承接口Map是多余的，这么做仅仅是为了让阅读代码的人明确知道HashMap是属于Map体系的，起到了文档的作用 AbstractMap相当于个辅助类，Map的一些操作这里面已经提供了默认实现，后面具体的子类如果没有特殊行为，可直接使用AbstractMap提供的实现。 Cloneable接口Cloneable这个接口设计的非常不好，最致命的一点是它里面竟然没有clone方法，也就是说我们自己写的类完全可以实现这个接口的同时不重写clone方法。关于Cloneable的不足，大家可以去看看《Effective Java》一书的作者给出的理由，在所给链接的文章里，Josh Bloch也会讲如何实现深拷贝比较好，我这里就不在赘述了。 Map接口在eclipse中的outline面板可以看到Map接口里面包含以下成员方法与内部类： Map_field_method可以看到，这里的成员方法不外乎是“增删改查”，这也反映了我们编写程序时，一定是以“数据”为导向的。 在上篇文章讲了Map虽然并不是Collection，但是它提供了三种“集合视角”(collection views)，与下面三个方法一一对应： Set keySet()，提供key的集合视角 Collection values()，提供value的集合视角 Set]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java集合框架综述]]></title>
      <url>%2F2017%2F04%2F19%2FJava%E9%9B%86%E5%90%88%E6%A1%86%E6%9E%B6%E7%BB%BC%E8%BF%B0%2F</url>
      <content type="text"><![CDATA[优秀的人，不是不合群，而是他们合群的人里面没有你最近被陆陆续续问了几遍HashMap的实现，回答的不好，打算复习复习JDK中的集合框架，并尝试分析其源码，这么做一方面是这些类非常实用，掌握其实现能更好的优化我们的程序；另一方面是学习借鉴JDK是如何实现了这么一套优雅高效的类库，提升编程能力。在介绍具体适合类之前，本篇文章对Java中的集合框架做一个大致描述，从一个高的角度俯视这个框架，了解了这个框架的一些理念与约定，会大大帮助后面分析某个具体类，让我们开始吧。 集合框架(collections framework)首先要明确，集合代表了一组对象(和数组一样，但数组长度不能变，而集合能)。Java中的集合框架定义了一套规范，用来表示、操作集合，使具体操作与实现细节解耦。其实说白了，可以把一个集合看成一个微型数据库，操作不外乎“增删改查”四种操作，我们在学习使用一个具体的集合类时，需要把这四个操作的时空复杂度弄清楚了，基本上就可以说掌握这个类了。 设计理念主要理念用一句话概括就是：提供一套“小而美”的API。API需要对程序员友好，增加新功能时能让程序员们快速上手。为了保证核心接口足够小，最顶层的接口(也就是Collection与Map接口)并不会区分该集合是否可变(mutability),是否可更改(modifiability),是否可改变大小(resizability)这些细微的差别。相反，一些操作是可选的，在实现时抛出UnsupportedOperationException即可表示集合不支持该操作。集合的实现者必须在文档中声明那些操作是不支持的。为了保证最顶层的核心接口足够小，它们只能包含下面情况下的方法： 基本操作，像之前说的“增删改查” There is a compelling performance reason why an important implementation would want to override it. 此外，所有的集合类都必须能提供友好的交互操作，这包括没有继承Collection类的数组对象。因此，框架提供一套方法，让集合类与数组可以相互转化，并且可以把Map看作成集合。 两大基类Collection与Map在集合框架的类继承体系中，最顶层有两个接口: Collection表示一组纯数据 Map表示一组key-value对 一般继承自Collection或Map的集合类，会提供两个“标准”的构造函数: 没有参数的构造函数，创建一个空的集合类 有一个类型与基类（Collection或Map）相同的构造函数，创建一个与给定参数具有相同元素的新集合类 因为接口中不能包含构造函数，所以上面这两个构造函数的约定并不是强制性的，但是在目前的集合框架中，所有继承自Collection或Map的子类都遵循这一约定。 Collection java-collection-hierarchy 如上图所示，Collection类主要有三个接口： Set表示不允许有重复元素的集合(A collection that contains no duplicate elements) List表示允许有重复元素的集合(An ordered collection (also known as a sequence)) Queue JDK1.5新增，与上面两个集合类主要是的区分在于Queue主要用于存储数据，而不是处理数据。(A collection designed for holding elements prior to processing.) Map MapClassHierarchy Map并不是一个真正意义上的集合(are not true collections)，但是这个接口提供了三种“集合视角”(collection views )，使得可以像操作集合一样操作它们，具体如下： 把map的内容看作key的集合(map’s contents to be viewed as a set of keys) 把map的内容看作value的集合(map’s contents to be viewed as a collection of values) 把map的内容看作key-value映射的集合(map’s contents to be viewed as a set of key-value mappings) 集合的实现(Collection Implementations)实现集合接口的类一般遵循&lt;实现方式&gt;+&lt;接口&gt;的命名方式，通用的集合实现类如下表： Interface Hash Table Resizable Array Balanced Tree Linked List Hash Table + Linked List Set HashSet TreeSet LinkedList List ArrayList LinkedList Deque ArrayDeque Map HashMap TreeMap LinkedHashMap 总结今天先开个头，后面会陆陆续续来一系列干货，Stay Tuned。需要说明一点，今后所有源码分析都将基于Oracle JDK 1.7.0_71，请知悉。 1234$ java -versionjava version"1.7.0_71"Java(TM) SE Runtime Environment (build 1.7.0_71-b14)Java HotSpot(TM) 64-Bit Server VM (build 24.71-b01, mixed mode) 参考 http://docs.oracle.com/javase/7/docs/technotes/guides/collections/overview.html https://en.wikipedia.org/wiki/Java_collections_framework]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>%2F2017%2F04%2F18%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
    </entry>

    
  
  
</search>
